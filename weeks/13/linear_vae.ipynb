{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "from math import isclose\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vector 80\n",
      "number of vectors 460\n",
      "length of augmented data: 5000\n",
      "shape of augmented data: (5000, 80)\n"
     ]
    }
   ],
   "source": [
    "in_file = \"../data/output_qep/ind_joins.csv\"\n",
    "\n",
    "#2^15 = 32k\n",
    "COMBINATIONS = 2**15\n",
    "WORKLOAD_SIZE = 1\n",
    "batch = 2**5\n",
    "\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(in_file,'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    n_operators = int(lines[0])\n",
    "    n_tables = int(lines[1])\n",
    "    lines = lines[2:]\n",
    "\n",
    "    data = np.array([line.split(',') for line in lines], dtype=float)\n",
    "    \n",
    "data_len = len(data[0])\n",
    "print(f\"length of vector {len(data[0])}\")\n",
    "print(f\"number of vectors {len(data)}\")\n",
    "\n",
    "number_of_samples = 5000\n",
    "\n",
    "queries = []\n",
    "for i in range(number_of_samples):\n",
    "    # get random indices to generate workloads\n",
    "    workload = np.array([np.random.randint(0, len(data)) for i in range(WORKLOAD_SIZE)])\n",
    "    # get the data from those indices\n",
    "    workload = np.take(data, workload, axis=0)\n",
    "    # average the workload\n",
    "    workload = np.mean(workload, axis=0)\n",
    "    # append those queries\n",
    "    queries.append(workload)\n",
    "\n",
    "data = np.array(queries)\n",
    "\n",
    "print(\"length of augmented data: {}\".format(len(data)))\n",
    "print(\"shape of augmented data: {}\".format(data.shape))\n",
    "\n",
    "\"\"\"\n",
    "# normalize the data\n",
    "data_max = np.max(data, axis = 0)\n",
    "data_min = np.min(data, axis = 0)\n",
    "data = np.divide((data - data_min), (data_max - data_min), where=(data_max - data_min)!=0)\n",
    "\"\"\"\n",
    "\n",
    "# take the log the cost to squash it\n",
    "data[:,n_operators + n_tables:] = np.log10(data[:,n_operators + n_tables:], where=(data[:,n_operators + n_tables:] > 1)) \n",
    "data[:,n_operators + n_tables:] = np.log10(data[:,n_operators + n_tables:], where=(data[:,n_operators + n_tables:] > 1))\n",
    "\n",
    "# convert the data into the expected tensor form\n",
    "data = [torch.Tensor(d) for d in data]\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# create our train test split\n",
    "pct = 0.9\n",
    "train = data[:int(len(data)* pct)]\n",
    "test = data[int(len(data)*pct):]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "        0.0000, 0.2100, 0.8238, 0.7815, 0.1885, 0.8236, 0.7030, 0.1204])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder Neural Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 66, 59, 52, 45, 38, 31, 24, 17, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=66, bias=True)\n",
       "      (1): BatchNorm1d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=66, out_features=59, bias=True)\n",
       "      (1): BatchNorm1d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=59, out_features=52, bias=True)\n",
       "      (1): BatchNorm1d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=52, out_features=45, bias=True)\n",
       "      (1): BatchNorm1d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=45, out_features=38, bias=True)\n",
       "      (1): BatchNorm1d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=38, out_features=31, bias=True)\n",
       "      (1): BatchNorm1d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=31, out_features=24, bias=True)\n",
       "      (1): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Linear(in_features=24, out_features=17, bias=True)\n",
       "      (1): BatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Linear(in_features=17, out_features=10, bias=True)\n",
       "      (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=17, bias=True)\n",
       "      (1): BatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=17, out_features=24, bias=True)\n",
       "      (1): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=24, out_features=31, bias=True)\n",
       "      (1): BatchNorm1d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=31, out_features=38, bias=True)\n",
       "      (1): BatchNorm1d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=38, out_features=45, bias=True)\n",
       "      (1): BatchNorm1d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=45, out_features=52, bias=True)\n",
       "      (1): BatchNorm1d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=52, out_features=59, bias=True)\n",
       "      (1): BatchNorm1d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Linear(in_features=59, out_features=66, bias=True)\n",
       "      (1): BatchNorm1d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Linear(in_features=66, out_features=80, bias=True)\n",
       "      (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (mu): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (sigma): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=80, out_features=80, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyVAE(nn.Module):\n",
    "    def __init__(self, latent_size = 10, number_of_layers = 10):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.layer_sizes = [self.latent_size + (data_len - self.latent_size) // (number_of_layers - 1) * \n",
    "                                    i for i in range(number_of_layers - 1)] + [data_len]\n",
    "        \n",
    "        self.layer_sizes = self.layer_sizes[::-1]\n",
    "        print(self.layer_sizes)\n",
    "        \n",
    "        self.encoder = []\n",
    "        self.decoder = []\n",
    "        \n",
    "        # Encoder\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            self.encoder.append(\n",
    "                nn.Sequential(\n",
    "                    #nn.BatchNorm1d(self.layer_sizes[i]),\n",
    "                    nn.Linear(self.layer_sizes[i], self.layer_sizes[i + 1]),\n",
    "                    nn.BatchNorm1d(self.layer_sizes[i + 1]),\n",
    "                    nn.LeakyReLU()))\n",
    "        self.encoder = nn.Sequential(*self.encoder)\n",
    "\n",
    "        # Decoder\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    #nn.BatchNorm1d(self.layer_sizes[~i]),\n",
    "                    nn.Linear(self.layer_sizes[~i], self.layer_sizes[~i - 1]),\n",
    "                    nn.BatchNorm1d(self.layer_sizes[~i - 1]),                    \n",
    "                    nn.LeakyReLU()))\n",
    "        self.decoder = nn.Sequential(*self.decoder)\n",
    "        \n",
    "        self.mu = nn.Linear(self.layer_sizes[-1], self.latent_size)\n",
    "        self.sigma = nn.Linear(self.layer_sizes[-1], self.latent_size)\n",
    "        self.fc = nn.Sequential(\n",
    "                    nn.Linear(data_len, data_len),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "    \n",
    "    def __print__(self):\n",
    "        return self.encoder + self.decoder\n",
    "    \n",
    "    def encode(self, x):\n",
    "        #print('encode x', x)\n",
    "        x = self.encoder(x)\n",
    "        #print('encode ex', x)\n",
    "        x = torch.flatten(x, start_dim = 1)\n",
    "        #print('encode fx', x)\n",
    "        return [self.mu(x), self.sigma(x)]\n",
    "    \n",
    "    def decode(self, z):\n",
    "        #print('decode z: ', z.shape)\n",
    "        #print('decode liz: ', z.shape)\n",
    "        #print('decode rsz: ', z.view(z.shape[0],z.shape[1],1).shape)\n",
    "        z = self.decoder(z)\n",
    "        #print('decode dz:  ', z.shape)\n",
    "        z = self.fc(z.squeeze(-1))\n",
    "        #print('decode fz:  ', z.shape)\n",
    "        return z\n",
    "    \n",
    "    def reparameterize(self, mu, sigma):\n",
    "        std = torch.exp(0.5 * sigma)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + sigma * eps\n",
    "        \n",
    "    \n",
    "    def loss(self, reconstructed_x, x, mu, log_var):\n",
    "        n = n_tables + n_operators\n",
    "\n",
    "        # BCE for the tables?\n",
    "        #bce_l1 = binarize(reconstructed_x[:n])\n",
    "        #bce_l2 = binarize(x[:n])\n",
    "        #reconstr_loss = F.binary_cross_entropy(torch.sigmoid(torch.tensor(bce_l1)), bce_l2)\n",
    "\n",
    "        # MSE for the costs?\n",
    "        #reconstr_loss += F.mse_loss(reconstructed_x[-7:], x[-7:])\n",
    "        \n",
    "        reconstr_loss = F.mse_loss(reconstructed_x, x)\n",
    "        kl_divergence = torch.mean(0.5 * torch.sum(log_var.exp() + mu ** 2 - 1 - log_var, dim = 1), dim = 0)\n",
    "        #print(f\"kl_divergence {kl_divergence}\")\n",
    "        #print(f\"reconstr_loss {reconstr_loss}\")\n",
    "\n",
    "        return kl_divergence*20 + reconstr_loss\n",
    "    \n",
    "    def sample(self, batch_size, eps = None):\n",
    "        z = torch.randn(batch_size, self.latent_size)\n",
    "        return self.decode(z)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print('forward x: ', x.shape)\n",
    "        mu, sigma = self.encode(x)\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        #print('forward rz: ', z.shape)\n",
    "        z = self.decode(z)\n",
    "        #print('forward dz: ', z.shape)\n",
    "        \n",
    "        return [z, x, mu, sigma]\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    \n",
    "vae = MyVAE()\n",
    "vae.train()\n",
    "#net.apply(initialize_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 59, 41, 23, 5]\n",
      "[epoch 1, i   20] loss: 2.658\n",
      "[epoch 1, i   40] loss: 1.704\n",
      "[epoch 1, i   60] loss: 1.235\n",
      "[epoch 1, i   80] loss: 0.977\n",
      "[epoch 1, i  100] loss: 0.774\n",
      "[epoch 1, i  120] loss: 0.626\n",
      "[epoch 1, i  140] loss: 0.494\n",
      "[epoch 2, i   20] loss: 0.399\n",
      "[epoch 2, i   40] loss: 0.319\n",
      "[epoch 2, i   60] loss: 0.258\n",
      "[epoch 2, i   80] loss: 0.215\n",
      "[epoch 2, i  100] loss: 0.181\n",
      "[epoch 2, i  120] loss: 0.169\n",
      "[epoch 2, i  140] loss: 0.138\n",
      "[epoch 3, i   20] loss: 0.128\n",
      "[epoch 3, i   40] loss: 0.115\n",
      "[epoch 3, i   60] loss: 0.100\n",
      "[epoch 3, i   80] loss: 0.087\n",
      "[epoch 3, i  100] loss: 0.086\n",
      "[epoch 3, i  120] loss: 0.069\n",
      "[epoch 3, i  140] loss: 0.082\n",
      "[epoch 4, i   20] loss: 0.065\n",
      "[epoch 4, i   40] loss: 0.059\n",
      "[epoch 4, i   60] loss: 0.055\n",
      "[epoch 4, i   80] loss: 0.057\n",
      "[epoch 4, i  100] loss: 0.048\n",
      "[epoch 4, i  120] loss: 0.046\n",
      "[epoch 4, i  140] loss: 0.045\n",
      "[epoch 5, i   20] loss: 0.040\n",
      "[epoch 5, i   40] loss: 0.036\n",
      "[epoch 5, i   60] loss: 0.040\n",
      "[epoch 5, i   80] loss: 0.037\n",
      "[epoch 5, i  100] loss: 0.033\n",
      "[epoch 5, i  120] loss: 0.035\n",
      "[epoch 5, i  140] loss: 0.029\n",
      "[epoch 6, i   20] loss: 0.030\n",
      "[epoch 6, i   40] loss: 0.028\n",
      "[epoch 6, i   60] loss: 0.026\n",
      "[epoch 6, i   80] loss: 0.028\n",
      "[epoch 6, i  100] loss: 0.030\n",
      "[epoch 6, i  120] loss: 0.029\n",
      "[epoch 6, i  140] loss: 0.026\n",
      "[epoch 7, i   20] loss: 0.023\n",
      "[epoch 7, i   40] loss: 0.030\n",
      "[epoch 7, i   60] loss: 0.022\n",
      "[epoch 7, i   80] loss: 0.022\n",
      "[epoch 7, i  100] loss: 0.024\n",
      "[epoch 7, i  120] loss: 0.023\n",
      "[epoch 7, i  140] loss: 0.021\n",
      "[epoch 8, i   20] loss: 0.020\n",
      "[epoch 8, i   40] loss: 0.022\n",
      "[epoch 8, i   60] loss: 0.020\n",
      "[epoch 8, i   80] loss: 0.021\n",
      "[epoch 8, i  100] loss: 0.026\n",
      "[epoch 8, i  120] loss: 0.021\n",
      "[epoch 8, i  140] loss: 0.021\n",
      "[epoch 9, i   20] loss: 0.019\n",
      "[epoch 9, i   40] loss: 0.019\n",
      "[epoch 9, i   60] loss: 0.023\n",
      "[epoch 9, i   80] loss: 0.019\n",
      "[epoch 9, i  100] loss: 0.017\n",
      "[epoch 9, i  120] loss: 0.019\n",
      "[epoch 9, i  140] loss: 0.016\n",
      "[epoch 10, i   20] loss: 0.018\n",
      "[epoch 10, i   40] loss: 0.018\n",
      "[epoch 10, i   60] loss: 0.018\n",
      "[epoch 10, i   80] loss: 0.019\n",
      "[epoch 10, i  100] loss: 0.018\n",
      "[epoch 10, i  120] loss: 0.016\n",
      "[epoch 10, i  140] loss: 0.019\n",
      "[epoch 11, i   20] loss: 0.020\n",
      "[epoch 11, i   40] loss: 0.016\n",
      "[epoch 11, i   60] loss: 0.018\n",
      "[epoch 11, i   80] loss: 0.015\n",
      "[epoch 11, i  100] loss: 0.019\n",
      "[epoch 11, i  120] loss: 0.018\n",
      "[epoch 11, i  140] loss: 0.015\n",
      "[epoch 12, i   20] loss: 0.014\n",
      "[epoch 12, i   40] loss: 0.016\n",
      "[epoch 12, i   60] loss: 0.020\n",
      "[epoch 12, i   80] loss: 0.016\n",
      "[epoch 12, i  100] loss: 0.014\n",
      "[epoch 12, i  120] loss: 0.016\n",
      "[epoch 12, i  140] loss: 0.018\n",
      "[epoch 13, i   20] loss: 0.016\n",
      "[epoch 13, i   40] loss: 0.015\n",
      "[epoch 13, i   60] loss: 0.013\n",
      "[epoch 13, i   80] loss: 0.015\n",
      "[epoch 13, i  100] loss: 0.016\n",
      "[epoch 13, i  120] loss: 0.015\n",
      "[epoch 13, i  140] loss: 0.016\n",
      "[epoch 14, i   20] loss: 0.016\n",
      "[epoch 14, i   40] loss: 0.015\n",
      "[epoch 14, i   60] loss: 0.015\n",
      "[epoch 14, i   80] loss: 0.012\n",
      "[epoch 14, i  100] loss: 0.014\n",
      "[epoch 14, i  120] loss: 0.014\n",
      "[epoch 14, i  140] loss: 0.014\n",
      "[epoch 15, i   20] loss: 0.015\n",
      "[epoch 15, i   40] loss: 0.014\n",
      "[epoch 15, i   60] loss: 0.014\n",
      "[epoch 15, i   80] loss: 0.013\n",
      "[epoch 15, i  100] loss: 0.013\n",
      "[epoch 15, i  120] loss: 0.015\n",
      "[epoch 15, i  140] loss: 0.015\n",
      "[epoch 16, i   20] loss: 0.014\n",
      "[epoch 16, i   40] loss: 0.014\n",
      "[epoch 16, i   60] loss: 0.013\n",
      "[epoch 16, i   80] loss: 0.015\n",
      "[epoch 16, i  100] loss: 0.015\n",
      "[epoch 16, i  120] loss: 0.011\n",
      "[epoch 16, i  140] loss: 0.013\n",
      "[epoch 17, i   20] loss: 0.012\n",
      "[epoch 17, i   40] loss: 0.014\n",
      "[epoch 17, i   60] loss: 0.014\n",
      "[epoch 17, i   80] loss: 0.013\n",
      "[epoch 17, i  100] loss: 0.013\n",
      "[epoch 17, i  120] loss: 0.013\n",
      "[epoch 17, i  140] loss: 0.014\n",
      "[epoch 18, i   20] loss: 0.013\n",
      "[epoch 18, i   40] loss: 0.013\n",
      "[epoch 18, i   60] loss: 0.014\n",
      "[epoch 18, i   80] loss: 0.012\n",
      "[epoch 18, i  100] loss: 0.013\n",
      "[epoch 18, i  120] loss: 0.014\n",
      "[epoch 18, i  140] loss: 0.011\n",
      "[epoch 19, i   20] loss: 0.011\n",
      "[epoch 19, i   40] loss: 0.014\n",
      "[epoch 19, i   60] loss: 0.012\n",
      "[epoch 19, i   80] loss: 0.012\n",
      "[epoch 19, i  100] loss: 0.012\n",
      "[epoch 19, i  120] loss: 0.012\n",
      "[epoch 19, i  140] loss: 0.015\n",
      "[epoch 20, i   20] loss: 0.013\n",
      "[epoch 20, i   40] loss: 0.011\n",
      "[epoch 20, i   60] loss: 0.013\n",
      "[epoch 20, i   80] loss: 0.012\n",
      "[epoch 20, i  100] loss: 0.014\n",
      "[epoch 20, i  120] loss: 0.011\n",
      "[epoch 20, i  140] loss: 0.011\n",
      "[epoch 21, i   20] loss: 0.012\n",
      "[epoch 21, i   40] loss: 0.012\n",
      "[epoch 21, i   60] loss: 0.013\n",
      "[epoch 21, i   80] loss: 0.012\n",
      "[epoch 21, i  100] loss: 0.012\n",
      "[epoch 21, i  120] loss: 0.011\n",
      "[epoch 21, i  140] loss: 0.011\n",
      "[epoch 22, i   20] loss: 0.011\n",
      "[epoch 22, i   40] loss: 0.015\n",
      "[epoch 22, i   60] loss: 0.012\n",
      "[epoch 22, i   80] loss: 0.010\n",
      "[epoch 22, i  100] loss: 0.011\n",
      "[epoch 22, i  120] loss: 0.012\n",
      "[epoch 22, i  140] loss: 0.010\n",
      "[epoch 23, i   20] loss: 0.011\n",
      "[epoch 23, i   40] loss: 0.011\n",
      "[epoch 23, i   60] loss: 0.010\n",
      "[epoch 23, i   80] loss: 0.010\n",
      "[epoch 23, i  100] loss: 0.012\n",
      "[epoch 23, i  120] loss: 0.011\n",
      "[epoch 23, i  140] loss: 0.012\n",
      "[epoch 24, i   20] loss: 0.011\n",
      "[epoch 24, i   40] loss: 0.012\n",
      "[epoch 24, i   60] loss: 0.011\n",
      "[epoch 24, i   80] loss: 0.011\n",
      "[epoch 24, i  100] loss: 0.011\n",
      "[epoch 24, i  120] loss: 0.010\n",
      "[epoch 24, i  140] loss: 0.012\n",
      "[epoch 25, i   20] loss: 0.011\n",
      "[epoch 25, i   40] loss: 0.012\n",
      "[epoch 25, i   60] loss: 0.011\n",
      "[epoch 25, i   80] loss: 0.011\n",
      "[epoch 25, i  100] loss: 0.014\n",
      "[epoch 25, i  120] loss: 0.009\n",
      "[epoch 25, i  140] loss: 0.009\n",
      "[epoch 26, i   20] loss: 0.010\n",
      "[epoch 26, i   40] loss: 0.012\n",
      "[epoch 26, i   60] loss: 0.011\n",
      "[epoch 26, i   80] loss: 0.010\n",
      "[epoch 26, i  100] loss: 0.012\n",
      "[epoch 26, i  120] loss: 0.010\n",
      "[epoch 26, i  140] loss: 0.011\n",
      "[epoch 27, i   20] loss: 0.014\n",
      "[epoch 27, i   40] loss: 0.010\n",
      "[epoch 27, i   60] loss: 0.011\n",
      "[epoch 27, i   80] loss: 0.009\n",
      "[epoch 27, i  100] loss: 0.010\n",
      "[epoch 27, i  120] loss: 0.009\n",
      "[epoch 27, i  140] loss: 0.011\n",
      "[epoch 28, i   20] loss: 0.009\n",
      "[epoch 28, i   40] loss: 0.009\n",
      "[epoch 28, i   60] loss: 0.009\n",
      "[epoch 28, i   80] loss: 0.009\n",
      "[epoch 28, i  100] loss: 0.009\n",
      "[epoch 28, i  120] loss: 0.011\n",
      "[epoch 28, i  140] loss: 0.013\n",
      "[epoch 29, i   20] loss: 0.009\n",
      "[epoch 29, i   40] loss: 0.010\n",
      "[epoch 29, i   60] loss: 0.009\n",
      "[epoch 29, i   80] loss: 0.011\n",
      "[epoch 29, i  100] loss: 0.011\n",
      "[epoch 29, i  120] loss: 0.010\n",
      "[epoch 29, i  140] loss: 0.011\n",
      "[epoch 30, i   20] loss: 0.009\n",
      "[epoch 30, i   40] loss: 0.011\n",
      "[epoch 30, i   60] loss: 0.009\n",
      "[epoch 30, i   80] loss: 0.011\n",
      "[epoch 30, i  100] loss: 0.009\n",
      "[epoch 30, i  120] loss: 0.010\n",
      "[epoch 30, i  140] loss: 0.009\n",
      "[epoch 31, i   20] loss: 0.009\n",
      "[epoch 31, i   40] loss: 0.011\n",
      "[epoch 31, i   60] loss: 0.010\n",
      "[epoch 31, i   80] loss: 0.010\n",
      "[epoch 31, i  100] loss: 0.009\n",
      "[epoch 31, i  120] loss: 0.007\n",
      "[epoch 31, i  140] loss: 0.012\n",
      "[epoch 32, i   20] loss: 0.008\n",
      "[epoch 32, i   40] loss: 0.010\n",
      "[epoch 32, i   60] loss: 0.013\n",
      "[epoch 32, i   80] loss: 0.008\n",
      "[epoch 32, i  100] loss: 0.008\n",
      "[epoch 32, i  120] loss: 0.011\n",
      "[epoch 32, i  140] loss: 0.010\n",
      "[epoch 33, i   20] loss: 0.013\n",
      "[epoch 33, i   40] loss: 0.009\n",
      "[epoch 33, i   60] loss: 0.010\n",
      "[epoch 33, i   80] loss: 0.009\n",
      "[epoch 33, i  100] loss: 0.008\n",
      "[epoch 33, i  120] loss: 0.009\n",
      "[epoch 33, i  140] loss: 0.010\n",
      "[epoch 34, i   20] loss: 0.008\n",
      "[epoch 34, i   40] loss: 0.010\n",
      "[epoch 34, i   60] loss: 0.009\n",
      "[epoch 34, i   80] loss: 0.010\n",
      "[epoch 34, i  100] loss: 0.010\n",
      "[epoch 34, i  120] loss: 0.010\n",
      "[epoch 34, i  140] loss: 0.010\n",
      "[epoch 35, i   20] loss: 0.007\n",
      "[epoch 35, i   40] loss: 0.007\n",
      "[epoch 35, i   60] loss: 0.010\n",
      "[epoch 35, i   80] loss: 0.009\n",
      "[epoch 35, i  100] loss: 0.012\n",
      "[epoch 35, i  120] loss: 0.008\n",
      "[epoch 35, i  140] loss: 0.011\n",
      "[epoch 36, i   20] loss: 0.008\n",
      "[epoch 36, i   40] loss: 0.007\n",
      "[epoch 36, i   60] loss: 0.008\n",
      "[epoch 36, i   80] loss: 0.011\n",
      "[epoch 36, i  100] loss: 0.011\n",
      "[epoch 36, i  120] loss: 0.010\n",
      "[epoch 36, i  140] loss: 0.011\n",
      "[epoch 37, i   20] loss: 0.008\n",
      "[epoch 37, i   40] loss: 0.008\n",
      "[epoch 37, i   60] loss: 0.008\n",
      "[epoch 37, i   80] loss: 0.012\n",
      "[epoch 37, i  100] loss: 0.010\n",
      "[epoch 37, i  120] loss: 0.008\n",
      "[epoch 37, i  140] loss: 0.009\n",
      "[epoch 38, i   20] loss: 0.008\n",
      "[epoch 38, i   40] loss: 0.011\n",
      "[epoch 38, i   60] loss: 0.008\n",
      "[epoch 38, i   80] loss: 0.009\n",
      "[epoch 38, i  100] loss: 0.010\n",
      "[epoch 38, i  120] loss: 0.009\n",
      "[epoch 38, i  140] loss: 0.008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 39, i   20] loss: 0.008\n",
      "[epoch 39, i   40] loss: 0.009\n",
      "[epoch 39, i   60] loss: 0.009\n",
      "[epoch 39, i   80] loss: 0.008\n",
      "[epoch 39, i  100] loss: 0.010\n",
      "[epoch 39, i  120] loss: 0.010\n",
      "[epoch 39, i  140] loss: 0.009\n",
      "[epoch 40, i   20] loss: 0.010\n",
      "[epoch 40, i   40] loss: 0.009\n",
      "[epoch 40, i   60] loss: 0.010\n",
      "[epoch 40, i   80] loss: 0.007\n",
      "[epoch 40, i  100] loss: 0.008\n",
      "[epoch 40, i  120] loss: 0.009\n",
      "[epoch 40, i  140] loss: 0.009\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "running_loss = 0\n",
    "number_of_epochs = 40\n",
    "display_frq = 20\n",
    "\n",
    "number_of_layers = 5\n",
    "latent_size = 5\n",
    "\n",
    "vae = MyVAE(latent_size, number_of_layers)\n",
    "vae.train()\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = batch, shuffle = True, num_workers = 4)\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    for i, d in enumerate(train_loader, 0):\n",
    "        if len(d) < batch:\n",
    "            break\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        reconstr_x, x, mu, sigma = vae(d)\n",
    "\n",
    "        #print('reconstr_x: ', reconstr_x.shape)\n",
    "        #print('x:        : ', x.shape)\n",
    "\n",
    "        loss = vae.loss(reconstr_x.unsqueeze(-1), x.unsqueeze(-1), mu, sigma)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # append the end of epoch numbers for plotting\n",
    "        if i % len(train_loader) == 0:\n",
    "            losses.append([len(losses), running_loss])\n",
    "        # display te loss every 'display_frq' inputs\n",
    "        if i % display_frq == display_frq - 1:\n",
    "            print('[epoch %d, i %4d] loss: %.3f' % (epoch + 1, i+ 1, running_loss / 100))\n",
    "            running_loss = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.94170494e+01]\n",
      " [2.12833166e+00]\n",
      " [9.00242805e-01]\n",
      " [2.54265070e-01]\n",
      " [1.64694667e-01]\n",
      " [1.19279183e-01]\n",
      " [1.60458207e-01]\n",
      " [1.17345951e-01]\n",
      " [8.23516622e-02]\n",
      " [6.98543489e-02]\n",
      " [8.45493674e-02]\n",
      " [8.71892944e-02]\n",
      " [6.40542507e-02]\n",
      " [6.37051165e-02]\n",
      " [4.66093086e-02]\n",
      " [3.94668467e-02]\n",
      " [4.08313982e-02]\n",
      " [4.62522618e-02]\n",
      " [5.66550344e-02]\n",
      " [7.97048137e-02]\n",
      " [4.03653048e-02]\n",
      " [5.77956587e-02]\n",
      " [3.56908180e-02]\n",
      " [3.51949558e-02]\n",
      " [4.72944379e-02]\n",
      " [3.17276530e-02]\n",
      " [6.34174943e-02]\n",
      " [4.46566641e-02]\n",
      " [3.96997891e-02]\n",
      " [2.91195437e-02]\n",
      " [4.14881185e-02]\n",
      " [2.01595910e-02]\n",
      " [5.38098030e-02]\n",
      " [7.73587823e-02]\n",
      " [4.74785045e-02]\n",
      " [2.33424939e-02]\n",
      " [3.54639255e-02]\n",
      " [3.77456509e-02]\n",
      " [2.90746503e-02]\n",
      " [1.77396685e-02]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x221c0a32190>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.array(losses)[:,1:])\n",
    "plt.plot(np.array(losses)[:,1:])\n",
    "#vae.save('20d_10l_vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vae.load_state_dict(torch.load('1d_vae'))\n",
    "#vae.load_state_dict(torch.load('10d_20l_vae'))\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = batch, shuffle = True, num_workers = 4)\n",
    "data_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = next(data_iter)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstr_x, x, mu, sigma = vae(data)\n",
    "    \n",
    "i = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80])\n",
      "torch.Size([80])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i += 1\n",
    "\n",
    "ox = x[i].squeeze(-1)\n",
    "rx = reconstr_x[i]\n",
    "plt.figure(figsize=(12,6))\n",
    "# Plot the data\n",
    "xlin = np.arange(data_len)\n",
    "\n",
    "#plt.plot(xlin,rx,'.-r', label='reconstructed data')\n",
    "#plt.plot(xlin,ox,'.-', label='original data')\n",
    "\n",
    "plt.bar(xlin,ox, width=0.95, label='original data', alpha=0.25)\n",
    "plt.bar(xlin,rx, width=0.95, label='reconstructed data', alpha=0.5)\n",
    "\n",
    "plt.legend(['original signal', 'reconstructed signal']);\n",
    "plt.axvline(x=float(n_operators) - 0.5)\n",
    "plt.axvline(x=float(n_tables + n_operators) - 0.5)\n",
    "\n",
    "print (ox.shape)\n",
    "print (rx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    INDEX = 0\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size = batch, shuffle = True, num_workers = 4)\n",
    "    data_iter = iter(test_loader)\n",
    "    # get one data pt\n",
    "    data = next(data_iter)\n",
    "\n",
    "    # get the lerp vals\n",
    "    lerp_offset = 1\n",
    "    lerp_range = 0.0\n",
    "    interp = [b * (1.0 / batch) * lerp_range + lerp_offset for b in range(batch)]\n",
    "\n",
    "    # get our latent space representations from the encoder\n",
    "    with torch.no_grad():\n",
    "        # encode\n",
    "        print('data shape:',data.shape)\n",
    "        mu, sigma = vae.encode(data)\n",
    "        # decode; lock a variable, permute, decode\n",
    "        z = vae.reparameterize(mu, sigma)\n",
    "\n",
    "    print(z.shape)\n",
    "    # get the first result; permute across the latent space\n",
    "    for i in range(batch):\n",
    "        z[i][INDEX] = interp[i]\n",
    "\n",
    "\n",
    "    # get ours reconstructions from the permuted latent spaces\n",
    "    with torch.no_grad():\n",
    "        results = vae.decode(z)\n",
    "\n",
    "    x = np.linspace(0,data_len,data_len)\n",
    "    y = np.linspace(0,batch,batch)\n",
    "\n",
    "    xc, yc = np.meshgrid(x, y)\n",
    "    res_surf = np.array(results)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    #YlOrRd\n",
    "    surf = ax.plot_surface(xc, yc, res_surf, cmap=\"coolwarm\",\n",
    "                           linewidth=0, antialiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    fig=plt.figure()\n",
    "    plt.ylim(top=15)\n",
    "\n",
    "    n=256 #Number of frames\n",
    "    x=range(56)\n",
    "    bars = plt.bar(x,results[0])\n",
    "\n",
    "    def animate(i):\n",
    "        for j, b in enumerate(bars):\n",
    "            b.set_height(results[i][j])\n",
    "\n",
    "    anim=animation.FuncAnimation(fig,animate,repeat=False,blit=False,frames=n,\n",
    "                                 interval=100)\n",
    "\n",
    "    anim.save('latent_perturbation.mp4',writer=animation.FFMpegWriter(fps=30))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "pt = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-embedding\n",
      "=============\n",
      "(close) similarity: 0.9175\n",
      "(far)   similarity: 0.3044\n",
      "\n",
      "post-embedding\n",
      "=============\n",
      "(close) similarity: 0.9080\n",
      "(far)   similarity: 0.4949\n"
     ]
    }
   ],
   "source": [
    "pt += 1\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    most_similar = 0\n",
    "    least_similar = None\n",
    "\n",
    "    # select a data point\n",
    "    d1 = data[pt]\n",
    "    d2 = None\n",
    "    d3 = None\n",
    "\n",
    "    vae.eval()\n",
    "\n",
    "    # get the closest and farthest vectors via cosine similarity\n",
    "    for i in range(1, len(data)):\n",
    "        similarity = cosine_sim(d1, data[i])\n",
    "        if similarity > most_similar and not np.isclose(1, similarity):\n",
    "            most_similar = similarity\n",
    "            d2 = data[i]\n",
    "        if least_similar is None or similarity < least_similar:\n",
    "            least_similar = similarity\n",
    "            d3 = data[i]\n",
    "\n",
    "    print(\"pre-embedding\")\n",
    "    print(\"=============\")\n",
    "    print(\"(close) similarity: %.4f\" % (cosine_sim(d1, d2)))\n",
    "    print(\"(far)   similarity: %.4f\" % (cosine_sim(d1, d3)))\n",
    "\n",
    "    # using the trained network, embed the points and\n",
    "    with torch.no_grad():\n",
    "        # get the embedded version of d1\n",
    "        d1 = torch.tensor(d1.reshape(1, data_len))\n",
    "        r1, _, _, _ = vae(d1)\n",
    "\n",
    "        d2 = torch.tensor(d2.reshape(1, data_len))\n",
    "        r2, _, _, _ = vae(d2)\n",
    "\n",
    "        d3 = torch.tensor(d3.reshape(1, data_len))\n",
    "        r3, _, _, _ = vae(d3)\n",
    "\n",
    "    d1r2 = cosine_sim(d1[0], r2[0])\n",
    "    d1r3 = cosine_sim(d1[0], r3[0])\n",
    "    print(\"\\npost-embedding\")\n",
    "    print(\"=============\")\n",
    "    print(\"(close) similarity: %.4f\" % d1r2)\n",
    "    print(\"(far)   similarity: %.4f\" % d1r3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
