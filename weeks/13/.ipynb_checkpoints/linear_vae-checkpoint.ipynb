{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "from math import isclose\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vector 80\n",
      "number of vectors 469\n",
      "length of augmented data: 899\n",
      "shape of augmented data: (899, 80)\n"
     ]
    }
   ],
   "source": [
    "in_file = \"../data/output_qep/ind_joins.csv\"\n",
    "\n",
    "#2^15 = 32k\n",
    "COMBINATIONS = 2**15\n",
    "WORKLOAD_SIZE = 1\n",
    "batch = 32\n",
    "\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(in_file,'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    n_operators = int(lines[0])\n",
    "    n_tables = int(lines[1])\n",
    "    lines = lines[2:]\n",
    "\n",
    "    data = np.array([line.split(',') for line in lines], dtype=float)\n",
    "    \n",
    "data_len = len(data[0])\n",
    "print(f\"length of vector {len(data[0])}\")\n",
    "print(f\"number of vectors {len(data)}\")\n",
    "\n",
    "number_of_samples = 899\n",
    "queries = []\n",
    "for i in range(number_of_samples):\n",
    "    # get random indices to generate workloads\n",
    "    workload = np.array([np.random.randint(0, len(data)) for i in range(WORKLOAD_SIZE)])\n",
    "    # get the data from those indices\n",
    "    workload = np.take(data, workload, axis=0)\n",
    "    # average the workload\n",
    "    workload = np.mean(workload, axis=0)\n",
    "    # append those queries\n",
    "    queries.append(workload)\n",
    "\n",
    "data = np.array(queries)\n",
    "\n",
    "print(\"length of augmented data: {}\".format(len(data)))\n",
    "print(\"shape of augmented data: {}\".format(data.shape))\n",
    "\n",
    "\"\"\"\n",
    "# normalize the data\n",
    "data_max = np.max(data, axis = 0)\n",
    "data_min = np.min(data, axis = 0)\n",
    "data = np.divide((data - data_min), (data_max - data_min), where=(data_max - data_min)!=0)\n",
    "\"\"\"\n",
    "\n",
    "# take the log the cost to squash it\n",
    "data[:,n_operators + n_tables:] = np.log10(data[:,n_operators + n_tables:], where=(data[:,n_operators + n_tables:] > 1)) \n",
    "data[:,n_operators + n_tables:] = np.log10(data[:,n_operators + n_tables:], where=(data[:,n_operators + n_tables:] > 1))\n",
    "\n",
    "# convert the data into the expected tensor form\n",
    "data = [torch.Tensor(d) for d in data]\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# create our train test split\n",
    "pct = 0.9\n",
    "train = data[:int(len(data)* pct)]\n",
    "test = data[int(len(data)*pct):]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0565, 0.6675, 0.3010, 0.0565, 0.6663, 0.3010, 0.0565])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder Neural Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 66, 59, 52, 45, 38, 31, 24, 17, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=66, bias=True)\n",
       "      (1): BatchNorm1d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=66, out_features=59, bias=True)\n",
       "      (1): BatchNorm1d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=59, out_features=52, bias=True)\n",
       "      (1): BatchNorm1d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=52, out_features=45, bias=True)\n",
       "      (1): BatchNorm1d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=45, out_features=38, bias=True)\n",
       "      (1): BatchNorm1d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=38, out_features=31, bias=True)\n",
       "      (1): BatchNorm1d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=31, out_features=24, bias=True)\n",
       "      (1): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Linear(in_features=24, out_features=17, bias=True)\n",
       "      (1): BatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Linear(in_features=17, out_features=10, bias=True)\n",
       "      (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=17, bias=True)\n",
       "      (1): BatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=17, out_features=24, bias=True)\n",
       "      (1): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=24, out_features=31, bias=True)\n",
       "      (1): BatchNorm1d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=31, out_features=38, bias=True)\n",
       "      (1): BatchNorm1d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=38, out_features=45, bias=True)\n",
       "      (1): BatchNorm1d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=45, out_features=52, bias=True)\n",
       "      (1): BatchNorm1d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=52, out_features=59, bias=True)\n",
       "      (1): BatchNorm1d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Linear(in_features=59, out_features=66, bias=True)\n",
       "      (1): BatchNorm1d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Linear(in_features=66, out_features=80, bias=True)\n",
       "      (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (mu): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (sigma): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=80, out_features=80, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyVAE(nn.Module):\n",
    "    def __init__(self, latent_size = 10, number_of_layers = 10):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.layer_sizes = [self.latent_size + (data_len - self.latent_size) // (number_of_layers - 1) * \n",
    "                                    i for i in range(number_of_layers - 1)] + [data_len]\n",
    "        \n",
    "        self.layer_sizes = self.layer_sizes[::-1]\n",
    "        print(self.layer_sizes)\n",
    "        \n",
    "        self.encoder = []\n",
    "        self.decoder = []\n",
    "        \n",
    "        # Encoder\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            self.encoder.append(\n",
    "                nn.Sequential(\n",
    "                    #nn.BatchNorm1d(self.layer_sizes[i]),\n",
    "                    nn.Linear(self.layer_sizes[i], self.layer_sizes[i + 1]),\n",
    "                    nn.BatchNorm1d(self.layer_sizes[i + 1]),\n",
    "                    nn.LeakyReLU()))\n",
    "        self.encoder = nn.Sequential(*self.encoder)\n",
    "\n",
    "        # Decoder\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    #nn.BatchNorm1d(self.layer_sizes[~i]),\n",
    "                    nn.Linear(self.layer_sizes[~i], self.layer_sizes[~i - 1]),\n",
    "                    nn.BatchNorm1d(self.layer_sizes[~i - 1]),                    \n",
    "                    nn.LeakyReLU()))\n",
    "        self.decoder = nn.Sequential(*self.decoder)\n",
    "        \n",
    "        self.mu = nn.Linear(self.layer_sizes[-1], self.latent_size)\n",
    "        self.sigma = nn.Linear(self.layer_sizes[-1], self.latent_size)\n",
    "        self.fc = nn.Sequential(\n",
    "                    nn.Linear(data_len, data_len),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "    \n",
    "    def __print__(self):\n",
    "        return self.encoder + self.decoder\n",
    "    \n",
    "    def encode(self, x):\n",
    "        #print('encode x', x)\n",
    "        x = self.encoder(x)\n",
    "        #print('encode ex', x)\n",
    "        x = torch.flatten(x, start_dim = 1)\n",
    "        #print('encode fx', x)\n",
    "        return [self.mu(x), self.sigma(x)]\n",
    "    \n",
    "    def decode(self, z):\n",
    "        #print('decode z: ', z.shape)\n",
    "        #print('decode liz: ', z.shape)\n",
    "        #print('decode rsz: ', z.view(z.shape[0],z.shape[1],1).shape)\n",
    "        z = self.decoder(z)\n",
    "        #print('decode dz:  ', z.shape)\n",
    "        z = self.fc(z.squeeze(-1))\n",
    "        #print('decode fz:  ', z.shape)\n",
    "        return z\n",
    "    \n",
    "    def reparameterize(self, mu, sigma):\n",
    "        std = torch.exp(0.5 * sigma)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + sigma * eps\n",
    "        \n",
    "    \n",
    "    def loss(self, reconstructed_x, x, mu, log_var):\n",
    "        n = n_tables + n_operators\n",
    "\n",
    "        # BCE for the tables?\n",
    "        #bce_l1 = binarize(reconstructed_x[:n])\n",
    "        #bce_l2 = binarize(x[:n])\n",
    "        #reconstr_loss = F.binary_cross_entropy(torch.sigmoid(torch.tensor(bce_l1)), bce_l2)\n",
    "\n",
    "        # MSE for the costs?\n",
    "        #reconstr_loss += F.mse_loss(reconstructed_x[-7:], x[-7:])\n",
    "        \n",
    "        reconstr_loss = F.mse_loss(reconstructed_x, x)\n",
    "        kl_divergence = torch.mean(0.5 * torch.sum(log_var.exp() + mu ** 2 - 1 - log_var, dim = 1), dim = 0)\n",
    "        \n",
    "        \n",
    "        print(f\"kl_divergence {kl_divergence}\")\n",
    "        print(f\"reconstr_loss {reconstr_loss}\")\n",
    "\n",
    "        return kl_divergence + reconstr_loss\n",
    "    \n",
    "    def sample(self, batch_size, eps = None):\n",
    "        z = torch.randn(batch_size, self.latent_size)\n",
    "        return self.decode(z)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print('forward x: ', x.shape)\n",
    "        mu, sigma = self.encode(x)\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        #print('forward rz: ', z.shape)\n",
    "        z = self.decode(z)\n",
    "        #print('forward dz: ', z.shape)\n",
    "        \n",
    "        return [z, x, mu, sigma]\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    \n",
    "vae = MyVAE()\n",
    "vae.train()\n",
    "#net.apply(initialize_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 61, 44, 27, 10]\n",
      "kl_divergence 1.572111964225769\n",
      "reconstr_loss 0.11166198551654816\n",
      "kl_divergence 1.3656907081604004\n",
      "reconstr_loss 0.10411667823791504\n",
      "kl_divergence 1.3869638442993164\n",
      "reconstr_loss 0.0986189991235733\n",
      "kl_divergence 1.1424379348754883\n",
      "reconstr_loss 0.09606529772281647\n",
      "kl_divergence 1.2470484972000122\n",
      "reconstr_loss 0.08759652823209763\n",
      "kl_divergence 1.0308053493499756\n",
      "reconstr_loss 0.08644622564315796\n",
      "kl_divergence 1.1110641956329346\n",
      "reconstr_loss 0.0818311795592308\n",
      "kl_divergence 1.0593976974487305\n",
      "reconstr_loss 0.07080177962779999\n",
      "kl_divergence 0.9521213173866272\n",
      "reconstr_loss 0.0709034651517868\n",
      "kl_divergence 0.9804558753967285\n",
      "reconstr_loss 0.06819716095924377\n",
      "kl_divergence 0.7703069448471069\n",
      "reconstr_loss 0.06670431047677994\n",
      "kl_divergence 0.6902354955673218\n",
      "reconstr_loss 0.0627366304397583\n",
      "kl_divergence 0.7646062970161438\n",
      "reconstr_loss 0.06484438478946686\n",
      "kl_divergence 0.7088733315467834\n",
      "reconstr_loss 0.056715209037065506\n",
      "kl_divergence 0.6963921785354614\n",
      "reconstr_loss 0.054638393223285675\n",
      "kl_divergence 0.7137160301208496\n",
      "reconstr_loss 0.05673602968454361\n",
      "kl_divergence 0.621036171913147\n",
      "reconstr_loss 0.04899370297789574\n",
      "kl_divergence 0.5367102026939392\n",
      "reconstr_loss 0.04808785766363144\n",
      "kl_divergence 0.6100159883499146\n",
      "reconstr_loss 0.04844116047024727\n",
      "kl_divergence 0.5961935520172119\n",
      "reconstr_loss 0.05038769915699959\n",
      "[epoch 1, i   20] loss: 0.200\n",
      "kl_divergence 0.628825306892395\n",
      "reconstr_loss 0.05044020339846611\n",
      "kl_divergence 0.577489972114563\n",
      "reconstr_loss 0.05359652638435364\n",
      "kl_divergence 0.47528159618377686\n",
      "reconstr_loss 0.051333796232938766\n",
      "kl_divergence 0.5170118808746338\n",
      "reconstr_loss 0.04551655054092407\n",
      "kl_divergence 0.5969643592834473\n",
      "reconstr_loss 0.048331763595342636\n",
      "kl_divergence 0.4374217092990875\n",
      "reconstr_loss 0.038609907031059265\n",
      "kl_divergence 0.5152754783630371\n",
      "reconstr_loss 0.0509391650557518\n",
      "kl_divergence 0.48954957723617554\n",
      "reconstr_loss 0.04262690618634224\n",
      "kl_divergence 0.4197100102901459\n",
      "reconstr_loss 0.04052356630563736\n",
      "kl_divergence 0.517144501209259\n",
      "reconstr_loss 0.04592011496424675\n",
      "kl_divergence 0.3671906590461731\n",
      "reconstr_loss 0.04117788374423981\n",
      "kl_divergence 0.37098434567451477\n",
      "reconstr_loss 0.04244662821292877\n",
      "kl_divergence 0.37840837240219116\n",
      "reconstr_loss 0.03775366395711899\n",
      "kl_divergence 0.33879750967025757\n",
      "reconstr_loss 0.038837797939777374\n",
      "kl_divergence 0.32860374450683594\n",
      "reconstr_loss 0.0381346270442009\n",
      "kl_divergence 0.3471450209617615\n",
      "reconstr_loss 0.037127189338207245\n",
      "kl_divergence 0.3339427411556244\n",
      "reconstr_loss 0.03514985740184784\n",
      "kl_divergence 0.304805189371109\n",
      "reconstr_loss 0.030711287632584572\n",
      "kl_divergence 0.3122654855251312\n",
      "reconstr_loss 0.0384938083589077\n",
      "kl_divergence 0.3478635549545288\n",
      "reconstr_loss 0.03960457071661949\n",
      "kl_divergence 0.2581445276737213\n",
      "reconstr_loss 0.03947126865386963\n",
      "kl_divergence 0.296109139919281\n",
      "reconstr_loss 0.033325742930173874\n",
      "kl_divergence 0.24365174770355225\n",
      "reconstr_loss 0.036061711609363556\n",
      "kl_divergence 0.36234772205352783\n",
      "reconstr_loss 0.03592746704816818\n",
      "kl_divergence 0.22619202733039856\n",
      "reconstr_loss 0.03352872654795647\n",
      "[epoch 2, i   20] loss: 0.110\n",
      "kl_divergence 0.3039091229438782\n",
      "reconstr_loss 0.03507602587342262\n",
      "kl_divergence 0.2360994815826416\n",
      "reconstr_loss 0.032751597464084625\n",
      "kl_divergence 0.2435445338487625\n",
      "reconstr_loss 0.035325147211551666\n",
      "kl_divergence 0.2630637586116791\n",
      "reconstr_loss 0.0328085795044899\n",
      "kl_divergence 0.24549813568592072\n",
      "reconstr_loss 0.026823733001947403\n",
      "kl_divergence 0.2655520439147949\n",
      "reconstr_loss 0.03157168626785278\n",
      "kl_divergence 0.17726345360279083\n",
      "reconstr_loss 0.028683727607131004\n",
      "kl_divergence 0.17588433623313904\n",
      "reconstr_loss 0.03258330374956131\n",
      "kl_divergence 0.17071284353733063\n",
      "reconstr_loss 0.037163056433200836\n",
      "kl_divergence 0.1797245293855667\n",
      "reconstr_loss 0.029413780197501183\n",
      "kl_divergence 0.18576748669147491\n",
      "reconstr_loss 0.03151370584964752\n",
      "kl_divergence 0.17521162331104279\n",
      "reconstr_loss 0.03272641450166702\n",
      "kl_divergence 0.242275208234787\n",
      "reconstr_loss 0.0403551310300827\n",
      "kl_divergence 0.17232206463813782\n",
      "reconstr_loss 0.024660596624016762\n",
      "kl_divergence 0.1526099145412445\n",
      "reconstr_loss 0.027171840891242027\n",
      "kl_divergence 0.18371640145778656\n",
      "reconstr_loss 0.027794549241662025\n",
      "kl_divergence 0.19670787453651428\n",
      "reconstr_loss 0.03565654903650284\n",
      "kl_divergence 0.17581038177013397\n",
      "reconstr_loss 0.0325191505253315\n",
      "kl_divergence 0.2024063766002655\n",
      "reconstr_loss 0.03494108468294144\n",
      "kl_divergence 0.1548970490694046\n",
      "reconstr_loss 0.030525323003530502\n",
      "kl_divergence 0.17969126999378204\n",
      "reconstr_loss 0.03283243998885155\n",
      "kl_divergence 0.1502656787633896\n",
      "reconstr_loss 0.024515176191926003\n",
      "kl_divergence 0.1513519287109375\n",
      "reconstr_loss 0.02896270714700222\n",
      "kl_divergence 0.12087643146514893\n",
      "reconstr_loss 0.02312612533569336\n",
      "kl_divergence 0.12667381763458252\n",
      "reconstr_loss 0.02484489604830742\n",
      "[epoch 3, i   20] loss: 0.056\n",
      "kl_divergence 0.14509707689285278\n",
      "reconstr_loss 0.023734061047434807\n",
      "kl_divergence 0.11478559672832489\n",
      "reconstr_loss 0.028513291850686073\n",
      "kl_divergence 0.13204067945480347\n",
      "reconstr_loss 0.031835123896598816\n",
      "kl_divergence 0.14240069687366486\n",
      "reconstr_loss 0.030229618772864342\n",
      "kl_divergence 0.16881859302520752\n",
      "reconstr_loss 0.03462688997387886\n",
      "kl_divergence 0.10516475141048431\n",
      "reconstr_loss 0.026221130043268204\n",
      "kl_divergence 0.18151213228702545\n",
      "reconstr_loss 0.0367494598031044\n",
      "kl_divergence 0.13845162093639374\n",
      "reconstr_loss 0.029180023819208145\n",
      "kl_divergence 0.10180661082267761\n",
      "reconstr_loss 0.03022964671254158\n",
      "kl_divergence 0.13365474343299866\n",
      "reconstr_loss 0.031077418476343155\n",
      "kl_divergence 0.1517713963985443\n",
      "reconstr_loss 0.03155868500471115\n",
      "kl_divergence 0.11044705659151077\n",
      "reconstr_loss 0.027956435456871986\n",
      "kl_divergence 0.0988488644361496\n",
      "reconstr_loss 0.032517388463020325\n",
      "kl_divergence 0.14023804664611816\n",
      "reconstr_loss 0.028508517891168594\n",
      "kl_divergence 0.10357692837715149\n",
      "reconstr_loss 0.032316237688064575\n",
      "kl_divergence 0.10093928128480911\n",
      "reconstr_loss 0.027408044785261154\n",
      "kl_divergence 0.11209738254547119\n",
      "reconstr_loss 0.021444974467158318\n",
      "kl_divergence 0.09736454486846924\n",
      "reconstr_loss 0.029109667986631393\n",
      "kl_divergence 0.10984053462743759\n",
      "reconstr_loss 0.02721511758863926\n",
      "kl_divergence 0.11226888000965118\n",
      "reconstr_loss 0.03006936050951481\n",
      "kl_divergence 0.09398063272237778\n",
      "reconstr_loss 0.02804476022720337\n",
      "kl_divergence 0.10062429308891296\n",
      "reconstr_loss 0.03300698846578598\n",
      "kl_divergence 0.11101919412612915\n",
      "reconstr_loss 0.028235385194420815\n",
      "kl_divergence 0.1103268638253212\n",
      "reconstr_loss 0.026024144142866135\n",
      "kl_divergence 0.1257493942975998\n",
      "reconstr_loss 0.025118717923760414\n",
      "[epoch 4, i   20] loss: 0.038\n",
      "kl_divergence 0.08292724937200546\n",
      "reconstr_loss 0.02347370982170105\n",
      "kl_divergence 0.08067969977855682\n",
      "reconstr_loss 0.024546947330236435\n",
      "kl_divergence 0.08568397909402847\n",
      "reconstr_loss 0.030427541583776474\n",
      "kl_divergence 0.10037095099687576\n",
      "reconstr_loss 0.02793162502348423\n",
      "kl_divergence 0.07382296025753021\n",
      "reconstr_loss 0.02514583244919777\n",
      "kl_divergence 0.07985644042491913\n",
      "reconstr_loss 0.021721862256526947\n",
      "kl_divergence 0.08752995729446411\n",
      "reconstr_loss 0.031275708228349686\n",
      "kl_divergence 0.08266833424568176\n",
      "reconstr_loss 0.026630837470293045\n",
      "kl_divergence 0.07053777575492859\n",
      "reconstr_loss 0.029670562595129013\n",
      "kl_divergence 0.06295911967754364\n",
      "reconstr_loss 0.02490363083779812\n",
      "kl_divergence 0.09630613774061203\n",
      "reconstr_loss 0.025853846222162247\n",
      "kl_divergence 0.10954275727272034\n",
      "reconstr_loss 0.03453101962804794\n",
      "kl_divergence 0.10733529925346375\n",
      "reconstr_loss 0.025562774389982224\n",
      "kl_divergence 0.09559028595685959\n",
      "reconstr_loss 0.032430484890937805\n",
      "kl_divergence 0.059338733553886414\n",
      "reconstr_loss 0.026407789438962936\n",
      "kl_divergence 0.06276902556419373\n",
      "reconstr_loss 0.022934673354029655\n",
      "kl_divergence 0.05480154603719711\n",
      "reconstr_loss 0.025658568367362022\n",
      "kl_divergence 0.07835987955331802\n",
      "reconstr_loss 0.029108595103025436\n",
      "kl_divergence 0.0680999681353569\n",
      "reconstr_loss 0.020273759961128235\n",
      "kl_divergence 0.08776115626096725\n",
      "reconstr_loss 0.029306679964065552\n",
      "kl_divergence 0.060169026255607605\n",
      "reconstr_loss 0.027657443657517433\n",
      "kl_divergence 0.07644058763980865\n",
      "reconstr_loss 0.021952185779809952\n",
      "kl_divergence 0.1430264115333557\n",
      "reconstr_loss 0.02509387768805027\n",
      "kl_divergence 0.0728352889418602\n",
      "reconstr_loss 0.027973975986242294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_divergence 0.06261568516492844\n",
      "reconstr_loss 0.030817117542028427\n",
      "[epoch 5, i   20] loss: 0.027\n",
      "kl_divergence 0.08018992841243744\n",
      "reconstr_loss 0.023857297375798225\n",
      "kl_divergence 0.08390031009912491\n",
      "reconstr_loss 0.030641555786132812\n",
      "kl_divergence 0.12493082880973816\n",
      "reconstr_loss 0.029119277372956276\n",
      "kl_divergence 0.05728989094495773\n",
      "reconstr_loss 0.028953438624739647\n",
      "kl_divergence 0.07548873871564865\n",
      "reconstr_loss 0.027222972363233566\n",
      "kl_divergence 0.08194295316934586\n",
      "reconstr_loss 0.026041660457849503\n",
      "kl_divergence 0.0685163363814354\n",
      "reconstr_loss 0.02926737070083618\n",
      "kl_divergence 0.06005049869418144\n",
      "reconstr_loss 0.02380061149597168\n",
      "kl_divergence 0.054501671344041824\n",
      "reconstr_loss 0.02229856327176094\n",
      "kl_divergence 0.048589035868644714\n",
      "reconstr_loss 0.025059860199689865\n",
      "kl_divergence 0.06836901605129242\n",
      "reconstr_loss 0.032941244542598724\n",
      "kl_divergence 0.06791405379772186\n",
      "reconstr_loss 0.03194408491253853\n",
      "kl_divergence 0.05124431103467941\n",
      "reconstr_loss 0.025159943848848343\n",
      "kl_divergence 0.07321590185165405\n",
      "reconstr_loss 0.022095177322626114\n",
      "kl_divergence 0.05540522560477257\n",
      "reconstr_loss 0.023150457069277763\n",
      "kl_divergence 0.06297546625137329\n",
      "reconstr_loss 0.027814870700240135\n",
      "kl_divergence 0.07785642892122269\n",
      "reconstr_loss 0.02909763716161251\n",
      "kl_divergence 0.05812801420688629\n",
      "reconstr_loss 0.025235498324036598\n",
      "kl_divergence 0.07320617139339447\n",
      "reconstr_loss 0.03016458824276924\n",
      "kl_divergence 0.05028405413031578\n",
      "reconstr_loss 0.028813257813453674\n",
      "kl_divergence 0.0655355453491211\n",
      "reconstr_loss 0.03101539984345436\n",
      "kl_divergence 0.05792831629514694\n",
      "reconstr_loss 0.02478162571787834\n",
      "kl_divergence 0.08858755975961685\n",
      "reconstr_loss 0.028271952643990517\n",
      "kl_divergence 0.09222748130559921\n",
      "reconstr_loss 0.02420736476778984\n",
      "kl_divergence 0.04339204728603363\n",
      "reconstr_loss 0.021233970299363136\n",
      "[epoch 6, i   20] loss: 0.024\n",
      "kl_divergence 0.06214859336614609\n",
      "reconstr_loss 0.03254801779985428\n",
      "kl_divergence 0.041893959045410156\n",
      "reconstr_loss 0.02402099408209324\n",
      "kl_divergence 0.057071685791015625\n",
      "reconstr_loss 0.02612726017832756\n",
      "kl_divergence 0.06922601163387299\n",
      "reconstr_loss 0.025078829377889633\n",
      "kl_divergence 0.05778222531080246\n",
      "reconstr_loss 0.02622392773628235\n",
      "kl_divergence 0.05366494506597519\n",
      "reconstr_loss 0.021598171442747116\n",
      "kl_divergence 0.04382338747382164\n",
      "reconstr_loss 0.0212696623057127\n",
      "kl_divergence 0.05413924157619476\n",
      "reconstr_loss 0.02279093861579895\n",
      "kl_divergence 0.04495503008365631\n",
      "reconstr_loss 0.029213309288024902\n",
      "kl_divergence 0.043761927634477615\n",
      "reconstr_loss 0.02705160342156887\n",
      "kl_divergence 0.04997197538614273\n",
      "reconstr_loss 0.023074660450220108\n",
      "kl_divergence 0.05064692348241806\n",
      "reconstr_loss 0.026087526232004166\n",
      "kl_divergence 0.05667298659682274\n",
      "reconstr_loss 0.023381579667329788\n",
      "kl_divergence 0.053496070206165314\n",
      "reconstr_loss 0.025673165917396545\n",
      "kl_divergence 0.05341317504644394\n",
      "reconstr_loss 0.02753254771232605\n",
      "kl_divergence 0.07877709716558456\n",
      "reconstr_loss 0.026783287525177002\n",
      "kl_divergence 0.07689142227172852\n",
      "reconstr_loss 0.03325020521879196\n",
      "kl_divergence 0.04993261769413948\n",
      "reconstr_loss 0.021258629858493805\n",
      "kl_divergence 0.04952327162027359\n",
      "reconstr_loss 0.02559787966310978\n",
      "kl_divergence 0.042094215750694275\n",
      "reconstr_loss 0.024409519508481026\n",
      "kl_divergence 0.07191111147403717\n",
      "reconstr_loss 0.03242701664566994\n",
      "kl_divergence 0.03868073970079422\n",
      "reconstr_loss 0.021938251331448555\n",
      "kl_divergence 0.03646407276391983\n",
      "reconstr_loss 0.022561196237802505\n",
      "kl_divergence 0.03666253015398979\n",
      "reconstr_loss 0.02289384789764881\n",
      "kl_divergence 0.05053773522377014\n",
      "reconstr_loss 0.03199038654565811\n",
      "[epoch 7, i   20] loss: 0.020\n",
      "kl_divergence 0.05034222453832626\n",
      "reconstr_loss 0.028285512700676918\n",
      "kl_divergence 0.05149880424141884\n",
      "reconstr_loss 0.023884352296590805\n",
      "kl_divergence 0.05238728225231171\n",
      "reconstr_loss 0.02727203629910946\n",
      "kl_divergence 0.037654999643564224\n",
      "reconstr_loss 0.022058406844735146\n",
      "kl_divergence 0.048500873148441315\n",
      "reconstr_loss 0.024096619337797165\n",
      "kl_divergence 0.04952093958854675\n",
      "reconstr_loss 0.02544783428311348\n",
      "kl_divergence 0.05003808066248894\n",
      "reconstr_loss 0.029096197336912155\n",
      "kl_divergence 0.04845230281352997\n",
      "reconstr_loss 0.02023746818304062\n",
      "kl_divergence 0.04927515238523483\n",
      "reconstr_loss 0.029851779341697693\n",
      "kl_divergence 0.05963879078626633\n",
      "reconstr_loss 0.024554912000894547\n",
      "kl_divergence 0.07276108860969543\n",
      "reconstr_loss 0.028670772910118103\n",
      "kl_divergence 0.035816553980112076\n",
      "reconstr_loss 0.023700665682554245\n",
      "kl_divergence 0.03286352753639221\n",
      "reconstr_loss 0.02416292019188404\n",
      "kl_divergence 0.05472198873758316\n",
      "reconstr_loss 0.025555694475769997\n",
      "kl_divergence 0.04991080239415169\n",
      "reconstr_loss 0.031196046620607376\n",
      "kl_divergence 0.041067831218242645\n",
      "reconstr_loss 0.0242462120950222\n",
      "kl_divergence 0.054747261106967926\n",
      "reconstr_loss 0.02707018330693245\n",
      "kl_divergence 0.045142482966184616\n",
      "reconstr_loss 0.027632329612970352\n",
      "kl_divergence 0.06238658353686333\n",
      "reconstr_loss 0.030451154336333275\n",
      "kl_divergence 0.03825768455862999\n",
      "reconstr_loss 0.022229155525565147\n",
      "kl_divergence 0.04839099198579788\n",
      "reconstr_loss 0.022980256006121635\n",
      "kl_divergence 0.050919510424137115\n",
      "reconstr_loss 0.030523162335157394\n",
      "kl_divergence 0.06766624003648758\n",
      "reconstr_loss 0.02738838829100132\n",
      "kl_divergence 0.03991726040840149\n",
      "reconstr_loss 0.025143170729279518\n",
      "kl_divergence 0.04320765286684036\n",
      "reconstr_loss 0.021323401480913162\n",
      "[epoch 8, i   20] loss: 0.019\n",
      "kl_divergence 0.03777161240577698\n",
      "reconstr_loss 0.025150666013360023\n",
      "kl_divergence 0.040593694895505905\n",
      "reconstr_loss 0.024190183728933334\n",
      "kl_divergence 0.04712298884987831\n",
      "reconstr_loss 0.023367978632450104\n",
      "kl_divergence 0.06327477097511292\n",
      "reconstr_loss 0.02999155782163143\n",
      "kl_divergence 0.03395906463265419\n",
      "reconstr_loss 0.022215545177459717\n",
      "kl_divergence 0.03422815725207329\n",
      "reconstr_loss 0.020872041583061218\n",
      "kl_divergence 0.03158782422542572\n",
      "reconstr_loss 0.02085934206843376\n",
      "kl_divergence 0.03181206434965134\n",
      "reconstr_loss 0.025582950562238693\n",
      "kl_divergence 0.030551910400390625\n",
      "reconstr_loss 0.02197771891951561\n",
      "kl_divergence 0.04490825533866882\n",
      "reconstr_loss 0.024711232632398605\n",
      "kl_divergence 0.05098210275173187\n",
      "reconstr_loss 0.03338121622800827\n",
      "kl_divergence 0.03739790618419647\n",
      "reconstr_loss 0.01885535567998886\n",
      "kl_divergence 0.037463147193193436\n",
      "reconstr_loss 0.026568565517663956\n",
      "kl_divergence 0.036652542650699615\n",
      "reconstr_loss 0.02104927971959114\n",
      "kl_divergence 0.027354765683412552\n",
      "reconstr_loss 0.023419592529535294\n",
      "kl_divergence 0.031533557921648026\n",
      "reconstr_loss 0.022970624268054962\n",
      "kl_divergence 0.05675935372710228\n",
      "reconstr_loss 0.03105013072490692\n",
      "kl_divergence 0.031934842467308044\n",
      "reconstr_loss 0.021933363750576973\n",
      "kl_divergence 0.03329598158597946\n",
      "reconstr_loss 0.026816288009285927\n",
      "kl_divergence 0.04505685716867447\n",
      "reconstr_loss 0.03331484645605087\n",
      "kl_divergence 0.03516056016087532\n",
      "reconstr_loss 0.02565728686749935\n",
      "kl_divergence 0.035291701555252075\n",
      "reconstr_loss 0.029525194317102432\n",
      "kl_divergence 0.040646929293870926\n",
      "reconstr_loss 0.026427317410707474\n",
      "kl_divergence 0.03135380893945694\n",
      "reconstr_loss 0.01968461647629738\n",
      "kl_divergence 0.03261240944266319\n",
      "reconstr_loss 0.02469298616051674\n",
      "[epoch 9, i   20] loss: 0.016\n",
      "kl_divergence 0.04589097574353218\n",
      "reconstr_loss 0.016908474266529083\n",
      "kl_divergence 0.025333216413855553\n",
      "reconstr_loss 0.022973839193582535\n",
      "kl_divergence 0.04247533902525902\n",
      "reconstr_loss 0.025502463802695274\n",
      "kl_divergence 0.04899068549275398\n",
      "reconstr_loss 0.026048626750707626\n",
      "kl_divergence 0.048571307212114334\n",
      "reconstr_loss 0.026962723582983017\n",
      "kl_divergence 0.05171096697449684\n",
      "reconstr_loss 0.024087633937597275\n",
      "kl_divergence 0.02876131236553192\n",
      "reconstr_loss 0.024607807397842407\n",
      "kl_divergence 0.04366910457611084\n",
      "reconstr_loss 0.028568293899297714\n",
      "kl_divergence 0.048672452569007874\n",
      "reconstr_loss 0.023858970031142235\n",
      "kl_divergence 0.04473128914833069\n",
      "reconstr_loss 0.0232655368745327\n",
      "kl_divergence 0.0356159433722496\n",
      "reconstr_loss 0.023531734943389893\n",
      "kl_divergence 0.04661118984222412\n",
      "reconstr_loss 0.030550306662917137\n",
      "kl_divergence 0.03388967737555504\n",
      "reconstr_loss 0.027760785073041916\n",
      "kl_divergence 0.03133535385131836\n",
      "reconstr_loss 0.02082546055316925\n",
      "kl_divergence 0.034587837755680084\n",
      "reconstr_loss 0.021007049828767776\n",
      "kl_divergence 0.029496675357222557\n",
      "reconstr_loss 0.015537800267338753\n",
      "kl_divergence 0.024648044258356094\n",
      "reconstr_loss 0.019003814086318016\n",
      "kl_divergence 0.03547421842813492\n",
      "reconstr_loss 0.025421198457479477\n",
      "kl_divergence 0.03132052347064018\n",
      "reconstr_loss 0.018910158425569534\n",
      "kl_divergence 0.03811272233724594\n",
      "reconstr_loss 0.02718796394765377\n",
      "kl_divergence 0.025261320173740387\n",
      "reconstr_loss 0.02068832889199257\n",
      "kl_divergence 0.03665631636977196\n",
      "reconstr_loss 0.024651343002915382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_divergence 0.04667653515934944\n",
      "reconstr_loss 0.03264353424310684\n",
      "kl_divergence 0.02486983686685562\n",
      "reconstr_loss 0.025539075955748558\n",
      "kl_divergence 0.03138061612844467\n",
      "reconstr_loss 0.026208773255348206\n",
      "[epoch 10, i   20] loss: 0.015\n",
      "kl_divergence 0.025839723646640778\n",
      "reconstr_loss 0.023119380697607994\n",
      "kl_divergence 0.032160721719264984\n",
      "reconstr_loss 0.01743490621447563\n",
      "kl_divergence 0.04558159410953522\n",
      "reconstr_loss 0.02044820412993431\n",
      "kl_divergence 0.025629747658967972\n",
      "reconstr_loss 0.02705901861190796\n",
      "kl_divergence 0.03362178057432175\n",
      "reconstr_loss 0.030254840850830078\n",
      "kl_divergence 0.03540477156639099\n",
      "reconstr_loss 0.01980219967663288\n",
      "kl_divergence 0.03465559706091881\n",
      "reconstr_loss 0.018629996106028557\n",
      "kl_divergence 0.03174930065870285\n",
      "reconstr_loss 0.029415134340524673\n",
      "kl_divergence 0.03753984719514847\n",
      "reconstr_loss 0.02510453388094902\n",
      "kl_divergence 0.023168230429291725\n",
      "reconstr_loss 0.02197849191725254\n",
      "kl_divergence 0.028435803949832916\n",
      "reconstr_loss 0.018620740622282028\n",
      "kl_divergence 0.03968276083469391\n",
      "reconstr_loss 0.02068881131708622\n",
      "kl_divergence 0.026427991688251495\n",
      "reconstr_loss 0.018581798300147057\n",
      "kl_divergence 0.027228936553001404\n",
      "reconstr_loss 0.020413944497704506\n",
      "kl_divergence 0.026227684691548347\n",
      "reconstr_loss 0.02257351763546467\n",
      "kl_divergence 0.023118047043681145\n",
      "reconstr_loss 0.02144491858780384\n",
      "kl_divergence 0.029544586315751076\n",
      "reconstr_loss 0.020891938358545303\n",
      "kl_divergence 0.03244078904390335\n",
      "reconstr_loss 0.02127249725162983\n",
      "kl_divergence 0.04301588609814644\n",
      "reconstr_loss 0.025406673550605774\n",
      "kl_divergence 0.026636384427547455\n",
      "reconstr_loss 0.018912069499492645\n",
      "kl_divergence 0.03777601197361946\n",
      "reconstr_loss 0.025257360190153122\n",
      "kl_divergence 0.028783293440937996\n",
      "reconstr_loss 0.028788095340132713\n",
      "kl_divergence 0.02333429455757141\n",
      "reconstr_loss 0.027520379051566124\n",
      "kl_divergence 0.033886607736349106\n",
      "reconstr_loss 0.020999381318688393\n",
      "kl_divergence 0.024199560284614563\n",
      "reconstr_loss 0.020168576389551163\n",
      "[epoch 11, i   20] loss: 0.013\n",
      "kl_divergence 0.024365436285734177\n",
      "reconstr_loss 0.019566284492611885\n",
      "kl_divergence 0.022593285888433456\n",
      "reconstr_loss 0.018293922767043114\n",
      "kl_divergence 0.04609246551990509\n",
      "reconstr_loss 0.027564778923988342\n",
      "kl_divergence 0.021067045629024506\n",
      "reconstr_loss 0.02216101437807083\n",
      "kl_divergence 0.03743097931146622\n",
      "reconstr_loss 0.021548161283135414\n",
      "kl_divergence 0.02602628618478775\n",
      "reconstr_loss 0.030187983065843582\n",
      "kl_divergence 0.028600411489605904\n",
      "reconstr_loss 0.017166409641504288\n",
      "kl_divergence 0.020074840635061264\n",
      "reconstr_loss 0.01800120435655117\n",
      "kl_divergence 0.02440374158322811\n",
      "reconstr_loss 0.019617125391960144\n",
      "kl_divergence 0.023946572095155716\n",
      "reconstr_loss 0.020119158551096916\n",
      "kl_divergence 0.024256229400634766\n",
      "reconstr_loss 0.021772919222712517\n",
      "kl_divergence 0.024339711293578148\n",
      "reconstr_loss 0.0210517980158329\n",
      "kl_divergence 0.026047419756650925\n",
      "reconstr_loss 0.023988014087080956\n",
      "kl_divergence 0.026771305128932\n",
      "reconstr_loss 0.024808231741189957\n",
      "kl_divergence 0.03964577615261078\n",
      "reconstr_loss 0.024263303726911545\n",
      "kl_divergence 0.02172178030014038\n",
      "reconstr_loss 0.020608393475413322\n",
      "kl_divergence 0.02149171754717827\n",
      "reconstr_loss 0.01808512583374977\n",
      "kl_divergence 0.025246046483516693\n",
      "reconstr_loss 0.020344378426671028\n",
      "kl_divergence 0.035653576254844666\n",
      "reconstr_loss 0.02049785479903221\n",
      "kl_divergence 0.04213875159621239\n",
      "reconstr_loss 0.032143861055374146\n",
      "kl_divergence 0.016482839360833168\n",
      "reconstr_loss 0.017500940710306168\n",
      "kl_divergence 0.039599932730197906\n",
      "reconstr_loss 0.028571102768182755\n",
      "kl_divergence 0.06424634158611298\n",
      "reconstr_loss 0.024878643453121185\n",
      "kl_divergence 0.0361679270863533\n",
      "reconstr_loss 0.025711435824632645\n",
      "kl_divergence 0.016575008630752563\n",
      "reconstr_loss 0.016049956902861595\n",
      "[epoch 12, i   20] loss: 0.013\n",
      "kl_divergence 0.03424591198563576\n",
      "reconstr_loss 0.024553820490837097\n",
      "kl_divergence 0.027106095105409622\n",
      "reconstr_loss 0.021051306277513504\n",
      "kl_divergence 0.02555006556212902\n",
      "reconstr_loss 0.02029854618012905\n",
      "kl_divergence 0.019995037466287613\n",
      "reconstr_loss 0.02228572405874729\n",
      "kl_divergence 0.030440520495176315\n",
      "reconstr_loss 0.02531861700117588\n",
      "kl_divergence 0.026182793080806732\n",
      "reconstr_loss 0.018885698169469833\n",
      "kl_divergence 0.024093663319945335\n",
      "reconstr_loss 0.02161528542637825\n",
      "kl_divergence 0.027878521010279655\n",
      "reconstr_loss 0.021386926993727684\n",
      "kl_divergence 0.02118930034339428\n",
      "reconstr_loss 0.024875419214367867\n",
      "kl_divergence 0.021077390760183334\n",
      "reconstr_loss 0.022852353751659393\n",
      "kl_divergence 0.02265668287873268\n",
      "reconstr_loss 0.02366580441594124\n",
      "kl_divergence 0.04315837472677231\n",
      "reconstr_loss 0.022351644933223724\n",
      "kl_divergence 0.021808721125125885\n",
      "reconstr_loss 0.022530095651745796\n",
      "kl_divergence 0.02364237979054451\n",
      "reconstr_loss 0.02437194623053074\n",
      "kl_divergence 0.020150696858763695\n",
      "reconstr_loss 0.022736914455890656\n",
      "kl_divergence 0.020829293876886368\n",
      "reconstr_loss 0.02032402902841568\n",
      "kl_divergence 0.019511062651872635\n",
      "reconstr_loss 0.016855617985129356\n",
      "kl_divergence 0.04590792953968048\n",
      "reconstr_loss 0.026831915602087975\n",
      "kl_divergence 0.02719416469335556\n",
      "reconstr_loss 0.018149854615330696\n",
      "kl_divergence 0.03070823848247528\n",
      "reconstr_loss 0.025765348225831985\n",
      "kl_divergence 0.03247762843966484\n",
      "reconstr_loss 0.022945493459701538\n",
      "kl_divergence 0.025777067989110947\n",
      "reconstr_loss 0.021920692175626755\n",
      "kl_divergence 0.026892641559243202\n",
      "reconstr_loss 0.021619459614157677\n",
      "kl_divergence 0.04533834755420685\n",
      "reconstr_loss 0.025375718250870705\n",
      "kl_divergence 0.054647766053676605\n",
      "reconstr_loss 0.020925888791680336\n",
      "[epoch 13, i   20] loss: 0.013\n",
      "kl_divergence 0.03877982497215271\n",
      "reconstr_loss 0.027052095159888268\n",
      "kl_divergence 0.020739935338497162\n",
      "reconstr_loss 0.01939292624592781\n",
      "kl_divergence 0.025125829502940178\n",
      "reconstr_loss 0.021025527268648148\n",
      "kl_divergence 0.03316739946603775\n",
      "reconstr_loss 0.015922293066978455\n",
      "kl_divergence 0.02423006296157837\n",
      "reconstr_loss 0.022775061428546906\n",
      "kl_divergence 0.02479860559105873\n",
      "reconstr_loss 0.020004797726869583\n",
      "kl_divergence 0.023496290668845177\n",
      "reconstr_loss 0.014310414902865887\n",
      "kl_divergence 0.026401665061712265\n",
      "reconstr_loss 0.021867498755455017\n",
      "kl_divergence 0.029453545808792114\n",
      "reconstr_loss 0.019981572404503822\n",
      "kl_divergence 0.0201730839908123\n",
      "reconstr_loss 0.024106252938508987\n",
      "kl_divergence 0.019932996481657028\n",
      "reconstr_loss 0.01811811700463295\n",
      "kl_divergence 0.01809682697057724\n",
      "reconstr_loss 0.019792906939983368\n",
      "kl_divergence 0.025946585461497307\n",
      "reconstr_loss 0.0257360078394413\n",
      "kl_divergence 0.019150394946336746\n",
      "reconstr_loss 0.021260429173707962\n",
      "kl_divergence 0.022695675492286682\n",
      "reconstr_loss 0.022746466100215912\n",
      "kl_divergence 0.07073087245225906\n",
      "reconstr_loss 0.026438552886247635\n",
      "kl_divergence 0.01688067428767681\n",
      "reconstr_loss 0.01229237113147974\n",
      "kl_divergence 0.028030790388584137\n",
      "reconstr_loss 0.024635817855596542\n",
      "kl_divergence 0.02124430239200592\n",
      "reconstr_loss 0.019122255966067314\n",
      "kl_divergence 0.023414097726345062\n",
      "reconstr_loss 0.019015248864889145\n",
      "kl_divergence 0.023249758407473564\n",
      "reconstr_loss 0.02389020100235939\n",
      "kl_divergence 0.030620982870459557\n",
      "reconstr_loss 0.03005116805434227\n",
      "kl_divergence 0.03351675719022751\n",
      "reconstr_loss 0.02568076178431511\n",
      "kl_divergence 0.04194783419370651\n",
      "reconstr_loss 0.025404680520296097\n",
      "kl_divergence 0.026333756744861603\n",
      "reconstr_loss 0.02598586119711399\n",
      "[epoch 14, i   20] loss: 0.012\n",
      "kl_divergence 0.017388872802257538\n",
      "reconstr_loss 0.019465414807200432\n",
      "kl_divergence 0.015517085790634155\n",
      "reconstr_loss 0.01899353414773941\n",
      "kl_divergence 0.03367657959461212\n",
      "reconstr_loss 0.029862936586141586\n",
      "kl_divergence 0.021704552695155144\n",
      "reconstr_loss 0.02283930405974388\n",
      "kl_divergence 0.032839447259902954\n",
      "reconstr_loss 0.026806926354765892\n",
      "kl_divergence 0.022045690566301346\n",
      "reconstr_loss 0.01933743618428707\n",
      "kl_divergence 0.020742949098348618\n",
      "reconstr_loss 0.025632306933403015\n",
      "kl_divergence 0.021421121433377266\n",
      "reconstr_loss 0.019465014338493347\n",
      "kl_divergence 0.03099273145198822\n",
      "reconstr_loss 0.02393214963376522\n",
      "kl_divergence 0.026776012033224106\n",
      "reconstr_loss 0.027097497135400772\n",
      "kl_divergence 0.018997076898813248\n",
      "reconstr_loss 0.02668265998363495\n",
      "kl_divergence 0.02311154082417488\n",
      "reconstr_loss 0.016005195677280426\n",
      "kl_divergence 0.020086029544472694\n",
      "reconstr_loss 0.017989739775657654\n",
      "kl_divergence 0.022489719092845917\n",
      "reconstr_loss 0.01932751014828682\n",
      "kl_divergence 0.02188974991440773\n",
      "reconstr_loss 0.02011070027947426\n",
      "kl_divergence 0.01986951194703579\n",
      "reconstr_loss 0.020704004913568497\n",
      "kl_divergence 0.032693084329366684\n",
      "reconstr_loss 0.02294355444610119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_divergence 0.01763085648417473\n",
      "reconstr_loss 0.017727630212903023\n",
      "kl_divergence 0.024904921650886536\n",
      "reconstr_loss 0.01824670098721981\n",
      "kl_divergence 0.01915341056883335\n",
      "reconstr_loss 0.018196796998381615\n",
      "kl_divergence 0.016655076295137405\n",
      "reconstr_loss 0.016171161085367203\n",
      "kl_divergence 0.018981048837304115\n",
      "reconstr_loss 0.016924425959587097\n",
      "kl_divergence 0.022982006892561913\n",
      "reconstr_loss 0.019493436440825462\n",
      "kl_divergence 0.017530925571918488\n",
      "reconstr_loss 0.01842290721833706\n",
      "kl_divergence 0.018528975546360016\n",
      "reconstr_loss 0.018144015222787857\n",
      "[epoch 15, i   20] loss: 0.011\n",
      "kl_divergence 0.018455425277352333\n",
      "reconstr_loss 0.016843974590301514\n",
      "kl_divergence 0.021848777309060097\n",
      "reconstr_loss 0.01660890318453312\n",
      "kl_divergence 0.017267238348722458\n",
      "reconstr_loss 0.016267897561192513\n",
      "kl_divergence 0.01672700233757496\n",
      "reconstr_loss 0.020571699365973473\n",
      "kl_divergence 0.014789711683988571\n",
      "reconstr_loss 0.02150152064859867\n",
      "kl_divergence 0.027988053858280182\n",
      "reconstr_loss 0.01972110942006111\n",
      "kl_divergence 0.02543766424059868\n",
      "reconstr_loss 0.021601540967822075\n",
      "kl_divergence 0.017792098224163055\n",
      "reconstr_loss 0.015498688444495201\n",
      "kl_divergence 0.014787865802645683\n",
      "reconstr_loss 0.016793975606560707\n",
      "kl_divergence 0.022184433415532112\n",
      "reconstr_loss 0.019670521840453148\n",
      "kl_divergence 0.01484418474137783\n",
      "reconstr_loss 0.018338806927204132\n",
      "kl_divergence 0.02845783531665802\n",
      "reconstr_loss 0.0251423679292202\n",
      "kl_divergence 0.015322721563279629\n",
      "reconstr_loss 0.017015036195516586\n",
      "kl_divergence 0.01587379164993763\n",
      "reconstr_loss 0.01980663649737835\n",
      "kl_divergence 0.016819056123495102\n",
      "reconstr_loss 0.01773727871477604\n",
      "kl_divergence 0.029869239777326584\n",
      "reconstr_loss 0.02820887602865696\n",
      "kl_divergence 0.015934035181999207\n",
      "reconstr_loss 0.018359852954745293\n",
      "kl_divergence 0.013912240043282509\n",
      "reconstr_loss 0.014509610831737518\n",
      "kl_divergence 0.0178650114685297\n",
      "reconstr_loss 0.014515044167637825\n",
      "kl_divergence 0.01473227795213461\n",
      "reconstr_loss 0.020027119666337967\n",
      "kl_divergence 0.019999375566840172\n",
      "reconstr_loss 0.017326878383755684\n",
      "kl_divergence 0.01315639540553093\n",
      "reconstr_loss 0.016431134194135666\n",
      "kl_divergence 0.01108818780630827\n",
      "reconstr_loss 0.01635916158556938\n",
      "kl_divergence 0.01668320596218109\n",
      "reconstr_loss 0.019840391352772713\n",
      "kl_divergence 0.026265176013112068\n",
      "reconstr_loss 0.024122735485434532\n",
      "[epoch 16, i   20] loss: 0.009\n",
      "kl_divergence 0.018950920552015305\n",
      "reconstr_loss 0.02159855328500271\n",
      "kl_divergence 0.020506080240011215\n",
      "reconstr_loss 0.021613363176584244\n",
      "kl_divergence 0.02352266013622284\n",
      "reconstr_loss 0.029002031311392784\n",
      "kl_divergence 0.011388854123651981\n",
      "reconstr_loss 0.012286121025681496\n",
      "kl_divergence 0.016105717048048973\n",
      "reconstr_loss 0.019516076892614365\n",
      "kl_divergence 0.022263074293732643\n",
      "reconstr_loss 0.019711200147867203\n",
      "kl_divergence 0.017041485756635666\n",
      "reconstr_loss 0.02645747736096382\n",
      "kl_divergence 0.017328549176454544\n",
      "reconstr_loss 0.019818630069494247\n",
      "kl_divergence 0.027255311608314514\n",
      "reconstr_loss 0.02691633626818657\n",
      "kl_divergence 0.019838865846395493\n",
      "reconstr_loss 0.0186769962310791\n",
      "kl_divergence 0.017271917313337326\n",
      "reconstr_loss 0.02352931909263134\n",
      "kl_divergence 0.01067547407001257\n",
      "reconstr_loss 0.016327571123838425\n",
      "kl_divergence 0.02106372080743313\n",
      "reconstr_loss 0.020145583897829056\n",
      "kl_divergence 0.014647919684648514\n",
      "reconstr_loss 0.020915020257234573\n",
      "kl_divergence 0.012773329392075539\n",
      "reconstr_loss 0.012987159192562103\n",
      "kl_divergence 0.02125978283584118\n",
      "reconstr_loss 0.020923256874084473\n",
      "kl_divergence 0.03052016720175743\n",
      "reconstr_loss 0.021490227431058884\n",
      "kl_divergence 0.016124902293086052\n",
      "reconstr_loss 0.018502740189433098\n",
      "kl_divergence 0.013708732090890408\n",
      "reconstr_loss 0.01787625625729561\n",
      "kl_divergence 0.01556523609906435\n",
      "reconstr_loss 0.012322420254349709\n",
      "kl_divergence 0.023360086604952812\n",
      "reconstr_loss 0.022651400417089462\n",
      "kl_divergence 0.03262161836028099\n",
      "reconstr_loss 0.03133594989776611\n",
      "kl_divergence 0.018161166459321976\n",
      "reconstr_loss 0.016567636281251907\n",
      "kl_divergence 0.026164740324020386\n",
      "reconstr_loss 0.021470410749316216\n",
      "kl_divergence 0.01155184768140316\n",
      "reconstr_loss 0.01643366925418377\n",
      "[epoch 17, i   20] loss: 0.010\n",
      "kl_divergence 0.01743033155798912\n",
      "reconstr_loss 0.014475986361503601\n",
      "kl_divergence 0.015988152474164963\n",
      "reconstr_loss 0.02189497835934162\n",
      "kl_divergence 0.02650155872106552\n",
      "reconstr_loss 0.021784832701086998\n",
      "kl_divergence 0.01764228381216526\n",
      "reconstr_loss 0.018362192437052727\n",
      "kl_divergence 0.019003067165613174\n",
      "reconstr_loss 0.019436050206422806\n",
      "kl_divergence 0.023004429414868355\n",
      "reconstr_loss 0.02580697275698185\n",
      "kl_divergence 0.018705686554312706\n",
      "reconstr_loss 0.01824083738029003\n",
      "kl_divergence 0.012748842127621174\n",
      "reconstr_loss 0.01619057171046734\n",
      "kl_divergence 0.018820542842149734\n",
      "reconstr_loss 0.01868891716003418\n",
      "kl_divergence 0.011845400556921959\n",
      "reconstr_loss 0.015297522768378258\n",
      "kl_divergence 0.011285806074738503\n",
      "reconstr_loss 0.022769516333937645\n",
      "kl_divergence 0.012210866436362267\n",
      "reconstr_loss 0.014368926174938679\n",
      "kl_divergence 0.01973743736743927\n",
      "reconstr_loss 0.02315768040716648\n",
      "kl_divergence 0.015277843922376633\n",
      "reconstr_loss 0.017313437536358833\n",
      "kl_divergence 0.015373853966593742\n",
      "reconstr_loss 0.023692574352025986\n",
      "kl_divergence 0.020015660673379898\n",
      "reconstr_loss 0.019646055996418\n",
      "kl_divergence 0.017094748094677925\n",
      "reconstr_loss 0.015182438306510448\n",
      "kl_divergence 0.016027411445975304\n",
      "reconstr_loss 0.019287921488285065\n",
      "kl_divergence 0.013108105398714542\n",
      "reconstr_loss 0.0160286333411932\n",
      "kl_divergence 0.009929260239005089\n",
      "reconstr_loss 0.015704786404967308\n",
      "kl_divergence 0.012542711570858955\n",
      "reconstr_loss 0.015345698222517967\n",
      "kl_divergence 0.01028752513229847\n",
      "reconstr_loss 0.016600966453552246\n",
      "kl_divergence 0.011966236867010593\n",
      "reconstr_loss 0.016472045332193375\n",
      "kl_divergence 0.009380292147397995\n",
      "reconstr_loss 0.01368778944015503\n",
      "kl_divergence 0.01188076101243496\n",
      "reconstr_loss 0.017059635370969772\n",
      "[epoch 18, i   20] loss: 0.008\n",
      "kl_divergence 0.012868190184235573\n",
      "reconstr_loss 0.021079212427139282\n",
      "kl_divergence 0.016064541414380074\n",
      "reconstr_loss 0.019481398165225983\n",
      "kl_divergence 0.013463037088513374\n",
      "reconstr_loss 0.016053926199674606\n",
      "kl_divergence 0.012720604427158833\n",
      "reconstr_loss 0.0214094165712595\n",
      "kl_divergence 0.013625051826238632\n",
      "reconstr_loss 0.01984556019306183\n",
      "kl_divergence 0.012802733108401299\n",
      "reconstr_loss 0.01574581488966942\n",
      "kl_divergence 0.009182261303067207\n",
      "reconstr_loss 0.010414914228022099\n",
      "kl_divergence 0.010008406825363636\n",
      "reconstr_loss 0.01637575402855873\n",
      "kl_divergence 0.011370936408638954\n",
      "reconstr_loss 0.018451545387506485\n",
      "kl_divergence 0.007793356664478779\n",
      "reconstr_loss 0.013390359468758106\n",
      "kl_divergence 0.011765047907829285\n",
      "reconstr_loss 0.025817472487688065\n",
      "kl_divergence 0.015520269051194191\n",
      "reconstr_loss 0.01924503780901432\n",
      "kl_divergence 0.011633972637355328\n",
      "reconstr_loss 0.02231490984559059\n",
      "kl_divergence 0.01659139059484005\n",
      "reconstr_loss 0.020000701770186424\n",
      "kl_divergence 0.01391330175101757\n",
      "reconstr_loss 0.024297576397657394\n",
      "kl_divergence 0.014674471691250801\n",
      "reconstr_loss 0.018378334119915962\n",
      "kl_divergence 0.01081512589007616\n",
      "reconstr_loss 0.015051563270390034\n",
      "kl_divergence 0.011080704629421234\n",
      "reconstr_loss 0.014533942565321922\n",
      "kl_divergence 0.010248441249132156\n",
      "reconstr_loss 0.01738942787051201\n",
      "kl_divergence 0.016975879669189453\n",
      "reconstr_loss 0.01835871860384941\n",
      "kl_divergence 0.012858547270298004\n",
      "reconstr_loss 0.019625570625066757\n",
      "kl_divergence 0.013614960946142673\n",
      "reconstr_loss 0.017769720405340195\n",
      "kl_divergence 0.00966059509664774\n",
      "reconstr_loss 0.009789520874619484\n",
      "kl_divergence 0.01860133931040764\n",
      "reconstr_loss 0.022059887647628784\n",
      "kl_divergence 0.011080842465162277\n",
      "reconstr_loss 0.015596146695315838\n",
      "[epoch 19, i   20] loss: 0.008\n",
      "kl_divergence 0.013030496425926685\n",
      "reconstr_loss 0.017510125413537025\n",
      "kl_divergence 0.013297896832227707\n",
      "reconstr_loss 0.014344158582389355\n",
      "kl_divergence 0.010545906610786915\n",
      "reconstr_loss 0.016628887504339218\n",
      "kl_divergence 0.010530708357691765\n",
      "reconstr_loss 0.013306811451911926\n",
      "kl_divergence 0.011201974004507065\n",
      "reconstr_loss 0.013624584302306175\n",
      "kl_divergence 0.01107671670615673\n",
      "reconstr_loss 0.016810676082968712\n",
      "kl_divergence 0.013046295382082462\n",
      "reconstr_loss 0.020766286179423332\n",
      "kl_divergence 0.01192739699035883\n",
      "reconstr_loss 0.01688859798014164\n",
      "kl_divergence 0.011706942692399025\n",
      "reconstr_loss 0.015028933063149452\n",
      "kl_divergence 0.009281840175390244\n",
      "reconstr_loss 0.015505136922001839\n",
      "kl_divergence 0.01079804077744484\n",
      "reconstr_loss 0.019716648384928703\n",
      "kl_divergence 0.010961344465613365\n",
      "reconstr_loss 0.014420224353671074\n",
      "kl_divergence 0.013311945833265781\n",
      "reconstr_loss 0.019072184339165688\n",
      "kl_divergence 0.01683897152543068\n",
      "reconstr_loss 0.02244420535862446\n",
      "kl_divergence 0.008334672078490257\n",
      "reconstr_loss 0.015317919664084911\n",
      "kl_divergence 0.007369917817413807\n",
      "reconstr_loss 0.019503360614180565\n",
      "kl_divergence 0.010483665391802788\n",
      "reconstr_loss 0.015974370762705803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_divergence 0.00806649960577488\n",
      "reconstr_loss 0.016902262344956398\n",
      "kl_divergence 0.015609107911586761\n",
      "reconstr_loss 0.017520170658826828\n",
      "kl_divergence 0.00767033826559782\n",
      "reconstr_loss 0.015103161334991455\n",
      "kl_divergence 0.014172583818435669\n",
      "reconstr_loss 0.0193481482565403\n",
      "kl_divergence 0.017484275624155998\n",
      "reconstr_loss 0.022463489323854446\n",
      "kl_divergence 0.013644453138113022\n",
      "reconstr_loss 0.012147014029324055\n",
      "kl_divergence 0.009019211865961552\n",
      "reconstr_loss 0.01302396785467863\n",
      "kl_divergence 0.019291505217552185\n",
      "reconstr_loss 0.024104822427034378\n",
      "[epoch 20, i   20] loss: 0.007\n",
      "kl_divergence 0.011205448769032955\n",
      "reconstr_loss 0.018131399527192116\n",
      "kl_divergence 0.008090006187558174\n",
      "reconstr_loss 0.013142114505171776\n",
      "kl_divergence 0.011406074278056622\n",
      "reconstr_loss 0.020547784864902496\n",
      "kl_divergence 0.013996738009154797\n",
      "reconstr_loss 0.013035503216087818\n",
      "kl_divergence 0.018803544342517853\n",
      "reconstr_loss 0.01695188321173191\n",
      "kl_divergence 0.010772032663226128\n",
      "reconstr_loss 0.019187316298484802\n",
      "kl_divergence 0.01128885243088007\n",
      "reconstr_loss 0.019345209002494812\n",
      "kl_divergence 0.008700650185346603\n",
      "reconstr_loss 0.010741503909230232\n",
      "kl_divergence 0.009109695442020893\n",
      "reconstr_loss 0.016157086938619614\n",
      "kl_divergence 0.006760031916201115\n",
      "reconstr_loss 0.016529273241758347\n",
      "kl_divergence 0.013300490565598011\n",
      "reconstr_loss 0.018614064902067184\n",
      "kl_divergence 0.014005517587065697\n",
      "reconstr_loss 0.017428459599614143\n",
      "kl_divergence 0.014985390938818455\n",
      "reconstr_loss 0.023290330544114113\n",
      "kl_divergence 0.012413145042955875\n",
      "reconstr_loss 0.017185848206281662\n",
      "kl_divergence 0.011929774656891823\n",
      "reconstr_loss 0.020263707265257835\n",
      "kl_divergence 0.008786654099822044\n",
      "reconstr_loss 0.011472419835627079\n",
      "kl_divergence 0.010780660435557365\n",
      "reconstr_loss 0.01578015275299549\n",
      "kl_divergence 0.008779975585639477\n",
      "reconstr_loss 0.01651984453201294\n",
      "kl_divergence 0.01426820270717144\n",
      "reconstr_loss 0.015407009050250053\n",
      "kl_divergence 0.012638520449399948\n",
      "reconstr_loss 0.017140354961156845\n",
      "kl_divergence 0.00859773252159357\n",
      "reconstr_loss 0.014434476383030415\n",
      "kl_divergence 0.012537078931927681\n",
      "reconstr_loss 0.012799146585166454\n",
      "kl_divergence 0.014405013993382454\n",
      "reconstr_loss 0.022631065919995308\n",
      "kl_divergence 0.02925422415137291\n",
      "reconstr_loss 0.02603098377585411\n",
      "kl_divergence 0.007213227916508913\n",
      "reconstr_loss 0.014940585009753704\n",
      "[epoch 21, i   20] loss: 0.007\n",
      "kl_divergence 0.013349942862987518\n",
      "reconstr_loss 0.024532053619623184\n",
      "kl_divergence 0.008500519208610058\n",
      "reconstr_loss 0.016149068251252174\n",
      "kl_divergence 0.01258905977010727\n",
      "reconstr_loss 0.019011307507753372\n",
      "kl_divergence 0.009969910606741905\n",
      "reconstr_loss 0.0144584896042943\n",
      "kl_divergence 0.010691642761230469\n",
      "reconstr_loss 0.012324349954724312\n",
      "kl_divergence 0.0097463084384799\n",
      "reconstr_loss 0.01788923516869545\n",
      "kl_divergence 0.010737678036093712\n",
      "reconstr_loss 0.012251384556293488\n",
      "kl_divergence 0.00845145434141159\n",
      "reconstr_loss 0.014094497077167034\n",
      "kl_divergence 0.010842335410416126\n",
      "reconstr_loss 0.014948325231671333\n",
      "kl_divergence 0.011433003470301628\n",
      "reconstr_loss 0.020076071843504906\n",
      "kl_divergence 0.010347167029976845\n",
      "reconstr_loss 0.01915200985968113\n",
      "kl_divergence 0.011330884881317616\n",
      "reconstr_loss 0.016977651044726372\n",
      "kl_divergence 0.011930561624467373\n",
      "reconstr_loss 0.012445898726582527\n",
      "kl_divergence 0.014893378131091595\n",
      "reconstr_loss 0.0258821751922369\n",
      "kl_divergence 0.010389843955636024\n",
      "reconstr_loss 0.014657223597168922\n",
      "kl_divergence 0.009471255354583263\n",
      "reconstr_loss 0.017927955836057663\n",
      "kl_divergence 0.01616837829351425\n",
      "reconstr_loss 0.015911828726530075\n",
      "kl_divergence 0.0117418821901083\n",
      "reconstr_loss 0.014537764713168144\n",
      "kl_divergence 0.00884812232106924\n",
      "reconstr_loss 0.014995647594332695\n",
      "kl_divergence 0.011066729202866554\n",
      "reconstr_loss 0.01733357086777687\n",
      "kl_divergence 0.00997902825474739\n",
      "reconstr_loss 0.015359980054199696\n",
      "kl_divergence 0.0072273509576916695\n",
      "reconstr_loss 0.012374171055853367\n",
      "kl_divergence 0.026618748903274536\n",
      "reconstr_loss 0.026699716225266457\n",
      "kl_divergence 0.008599883876740932\n",
      "reconstr_loss 0.019111385568976402\n",
      "kl_divergence 0.009140859358012676\n",
      "reconstr_loss 0.020777523517608643\n",
      "[epoch 22, i   20] loss: 0.007\n",
      "kl_divergence 0.009963628835976124\n",
      "reconstr_loss 0.014766720123589039\n",
      "kl_divergence 0.009186998941004276\n",
      "reconstr_loss 0.010548941791057587\n",
      "kl_divergence 0.026700081303715706\n",
      "reconstr_loss 0.03182254359126091\n",
      "kl_divergence 0.009432674385607243\n",
      "reconstr_loss 0.018050068989396095\n",
      "kl_divergence 0.01158416923135519\n",
      "reconstr_loss 0.019215550273656845\n",
      "kl_divergence 0.010231224820017815\n",
      "reconstr_loss 0.015632808208465576\n",
      "kl_divergence 0.008844859898090363\n",
      "reconstr_loss 0.01713481917977333\n",
      "kl_divergence 0.016034217551350594\n",
      "reconstr_loss 0.028153862804174423\n",
      "kl_divergence 0.008500496856868267\n",
      "reconstr_loss 0.012479089200496674\n",
      "kl_divergence 0.006866223178803921\n",
      "reconstr_loss 0.0166823361068964\n",
      "kl_divergence 0.008324434980750084\n",
      "reconstr_loss 0.013174660503864288\n",
      "kl_divergence 0.010097728110849857\n",
      "reconstr_loss 0.018620053306221962\n",
      "kl_divergence 0.00804317556321621\n",
      "reconstr_loss 0.016210462898015976\n",
      "kl_divergence 0.011959083378314972\n",
      "reconstr_loss 0.016746297478675842\n",
      "kl_divergence 0.007971926592290401\n",
      "reconstr_loss 0.01781664788722992\n",
      "kl_divergence 0.007068135775625706\n",
      "reconstr_loss 0.015508698299527168\n",
      "kl_divergence 0.006773249711841345\n",
      "reconstr_loss 0.010364331305027008\n",
      "kl_divergence 0.012138960883021355\n",
      "reconstr_loss 0.014967051334679127\n",
      "kl_divergence 0.010867336764931679\n",
      "reconstr_loss 0.014360347762703896\n",
      "kl_divergence 0.008606869727373123\n",
      "reconstr_loss 0.01227041520178318\n",
      "kl_divergence 0.008723076432943344\n",
      "reconstr_loss 0.0181544441729784\n",
      "kl_divergence 0.0067349267192184925\n",
      "reconstr_loss 0.010428803972899914\n",
      "kl_divergence 0.008776369504630566\n",
      "reconstr_loss 0.017102830111980438\n",
      "kl_divergence 0.008095135912299156\n",
      "reconstr_loss 0.01613406278192997\n",
      "kl_divergence 0.007811081595718861\n",
      "reconstr_loss 0.015728633850812912\n",
      "[epoch 23, i   20] loss: 0.007\n",
      "kl_divergence 0.009147154167294502\n",
      "reconstr_loss 0.011652648448944092\n",
      "kl_divergence 0.008024720475077629\n",
      "reconstr_loss 0.014877463690936565\n",
      "kl_divergence 0.007652719970792532\n",
      "reconstr_loss 0.016259944066405296\n",
      "kl_divergence 0.008698416873812675\n",
      "reconstr_loss 0.015224650502204895\n",
      "kl_divergence 0.0075081330724060535\n",
      "reconstr_loss 0.01983049511909485\n",
      "kl_divergence 0.008017981424927711\n",
      "reconstr_loss 0.01750435307621956\n",
      "kl_divergence 0.006727051921188831\n",
      "reconstr_loss 0.017572632059454918\n",
      "kl_divergence 0.006821374408900738\n",
      "reconstr_loss 0.013791151344776154\n",
      "kl_divergence 0.006442059762775898\n",
      "reconstr_loss 0.01298481784760952\n",
      "kl_divergence 0.01659298688173294\n",
      "reconstr_loss 0.023075608536601067\n",
      "kl_divergence 0.01193008478730917\n",
      "reconstr_loss 0.022346440702676773\n",
      "kl_divergence 0.010207080282270908\n",
      "reconstr_loss 0.019277725368738174\n",
      "kl_divergence 0.008490308187901974\n",
      "reconstr_loss 0.015775591135025024\n",
      "kl_divergence 0.006342220585793257\n",
      "reconstr_loss 0.010736343450844288\n",
      "kl_divergence 0.008827753365039825\n",
      "reconstr_loss 0.016343262046575546\n",
      "kl_divergence 0.006860568653792143\n",
      "reconstr_loss 0.018143106251955032\n",
      "kl_divergence 0.010848233476281166\n",
      "reconstr_loss 0.013744342140853405\n",
      "kl_divergence 0.008559392765164375\n",
      "reconstr_loss 0.012239808216691017\n",
      "kl_divergence 0.011142926290631294\n",
      "reconstr_loss 0.019687946885824203\n",
      "kl_divergence 0.00941244326531887\n",
      "reconstr_loss 0.017030607908964157\n",
      "kl_divergence 0.01256808266043663\n",
      "reconstr_loss 0.021754849702119827\n",
      "kl_divergence 0.008639516308903694\n",
      "reconstr_loss 0.01875946670770645\n",
      "kl_divergence 0.008308174088597298\n",
      "reconstr_loss 0.016324948519468307\n",
      "kl_divergence 0.008766655810177326\n",
      "reconstr_loss 0.017340052872896194\n",
      "kl_divergence 0.0070094517432153225\n",
      "reconstr_loss 0.016037045046687126\n",
      "[epoch 24, i   20] loss: 0.006\n",
      "kl_divergence 0.0068806493654847145\n",
      "reconstr_loss 0.017516078427433968\n",
      "kl_divergence 0.010076089762151241\n",
      "reconstr_loss 0.016071362420916557\n",
      "kl_divergence 0.006003777030855417\n",
      "reconstr_loss 0.017769882455468178\n",
      "kl_divergence 0.00717155821621418\n",
      "reconstr_loss 0.022510403767228127\n",
      "kl_divergence 0.012066147290170193\n",
      "reconstr_loss 0.01965663582086563\n",
      "kl_divergence 0.008632204495370388\n",
      "reconstr_loss 0.015617286786437035\n",
      "kl_divergence 0.009233329445123672\n",
      "reconstr_loss 0.012313713319599628\n",
      "kl_divergence 0.0076836789958179\n",
      "reconstr_loss 0.016061779111623764\n",
      "kl_divergence 0.006925992667675018\n",
      "reconstr_loss 0.013288432732224464\n",
      "kl_divergence 0.01277016568928957\n",
      "reconstr_loss 0.01880158670246601\n",
      "kl_divergence 0.010871130973100662\n",
      "reconstr_loss 0.01961561106145382\n",
      "kl_divergence 0.009410547092556953\n",
      "reconstr_loss 0.012800732627511024\n",
      "kl_divergence 0.0071462905034422874\n",
      "reconstr_loss 0.011277482844889164\n",
      "kl_divergence 0.009627111256122589\n",
      "reconstr_loss 0.014791443943977356\n",
      "kl_divergence 0.010853688232600689\n",
      "reconstr_loss 0.022621149197220802\n",
      "kl_divergence 0.011450140736997128\n",
      "reconstr_loss 0.019796278327703476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_divergence 0.01121445745229721\n",
      "reconstr_loss 0.018806684762239456\n",
      "kl_divergence 0.015827009454369545\n",
      "reconstr_loss 0.022571368142962456\n",
      "kl_divergence 0.006232467480003834\n",
      "reconstr_loss 0.011987963691353798\n",
      "kl_divergence 0.008683578111231327\n",
      "reconstr_loss 0.020923756062984467\n",
      "kl_divergence 0.009780656546354294\n",
      "reconstr_loss 0.01593552716076374\n",
      "kl_divergence 0.012273955158889294\n",
      "reconstr_loss 0.014744594693183899\n",
      "kl_divergence 0.010550569742918015\n",
      "reconstr_loss 0.015208102762699127\n",
      "kl_divergence 0.009896858595311642\n",
      "reconstr_loss 0.014940838329494\n",
      "kl_divergence 0.0061662765219807625\n",
      "reconstr_loss 0.01835847832262516\n",
      "[epoch 25, i   20] loss: 0.007\n",
      "kl_divergence 0.007601666729897261\n",
      "reconstr_loss 0.016978558152914047\n",
      "kl_divergence 0.01342921145260334\n",
      "reconstr_loss 0.020653540268540382\n",
      "kl_divergence 0.006055119913071394\n",
      "reconstr_loss 0.0166175439953804\n",
      "kl_divergence 0.007477485109120607\n",
      "reconstr_loss 0.015598319470882416\n",
      "kl_divergence 0.010340237990021706\n",
      "reconstr_loss 0.01611015573143959\n",
      "kl_divergence 0.006876541301608086\n",
      "reconstr_loss 0.01246308907866478\n",
      "kl_divergence 0.006519035901874304\n",
      "reconstr_loss 0.012959522195160389\n",
      "kl_divergence 0.006393733900040388\n",
      "reconstr_loss 0.011244816705584526\n",
      "kl_divergence 0.00721681909635663\n",
      "reconstr_loss 0.015429722145199776\n",
      "kl_divergence 0.024335190653800964\n",
      "reconstr_loss 0.028794297948479652\n",
      "kl_divergence 0.009141855873167515\n",
      "reconstr_loss 0.021045329049229622\n",
      "kl_divergence 0.00550498440861702\n",
      "reconstr_loss 0.018472803756594658\n",
      "kl_divergence 0.005694134160876274\n",
      "reconstr_loss 0.010455721989274025\n",
      "kl_divergence 0.012196065858006477\n",
      "reconstr_loss 0.022652512416243553\n",
      "kl_divergence 0.01421591080725193\n",
      "reconstr_loss 0.021294008940458298\n",
      "kl_divergence 0.005280935205519199\n",
      "reconstr_loss 0.015891198068857193\n",
      "kl_divergence 0.005874513648450375\n",
      "reconstr_loss 0.013366302475333214\n",
      "kl_divergence 0.007935058325529099\n",
      "reconstr_loss 0.014326894655823708\n",
      "kl_divergence 0.005832524970173836\n",
      "reconstr_loss 0.01361942756921053\n",
      "kl_divergence 0.007535000331699848\n",
      "reconstr_loss 0.011544990353286266\n",
      "kl_divergence 0.007150525227189064\n",
      "reconstr_loss 0.018762703984975815\n",
      "kl_divergence 0.00891171582043171\n",
      "reconstr_loss 0.015524742193520069\n",
      "kl_divergence 0.0066018300130963326\n",
      "reconstr_loss 0.009642897173762321\n",
      "kl_divergence 0.006070337258279324\n",
      "reconstr_loss 0.0150159802287817\n",
      "kl_divergence 0.005935236345976591\n",
      "reconstr_loss 0.017698032781481743\n",
      "[epoch 26, i   20] loss: 0.006\n",
      "kl_divergence 0.005891839042305946\n",
      "reconstr_loss 0.013352354988455772\n",
      "kl_divergence 0.014473360031843185\n",
      "reconstr_loss 0.01976116932928562\n",
      "kl_divergence 0.012467259541153908\n",
      "reconstr_loss 0.027306094765663147\n",
      "kl_divergence 0.008899146690964699\n",
      "reconstr_loss 0.01562134362757206\n",
      "kl_divergence 0.012652781791985035\n",
      "reconstr_loss 0.01749565824866295\n",
      "kl_divergence 0.00780271552503109\n",
      "reconstr_loss 0.016194626688957214\n",
      "kl_divergence 0.008259234018623829\n",
      "reconstr_loss 0.015736687928438187\n",
      "kl_divergence 0.006502295378595591\n",
      "reconstr_loss 0.01616441272199154\n",
      "kl_divergence 0.004623219836503267\n",
      "reconstr_loss 0.012410756200551987\n",
      "kl_divergence 0.008690212853252888\n",
      "reconstr_loss 0.018679698929190636\n",
      "kl_divergence 0.010173049755394459\n",
      "reconstr_loss 0.018371421843767166\n",
      "kl_divergence 0.006520254071801901\n",
      "reconstr_loss 0.011472577229142189\n",
      "kl_divergence 0.006354875862598419\n",
      "reconstr_loss 0.01440376602113247\n",
      "kl_divergence 0.006210838444530964\n",
      "reconstr_loss 0.01853518933057785\n",
      "kl_divergence 0.006283405236899853\n",
      "reconstr_loss 0.016919076442718506\n",
      "kl_divergence 0.01009723637253046\n",
      "reconstr_loss 0.015552033670246601\n",
      "kl_divergence 0.006460138596594334\n",
      "reconstr_loss 0.019766435027122498\n",
      "kl_divergence 0.006047358270734549\n",
      "reconstr_loss 0.011951005086302757\n",
      "kl_divergence 0.012003868818283081\n",
      "reconstr_loss 0.018044758588075638\n",
      "kl_divergence 0.00760675361379981\n",
      "reconstr_loss 0.015917794778943062\n",
      "kl_divergence 0.008090347051620483\n",
      "reconstr_loss 0.01732444204390049\n",
      "kl_divergence 0.005980240181088448\n",
      "reconstr_loss 0.012697611935436726\n",
      "kl_divergence 0.009659186005592346\n",
      "reconstr_loss 0.020160743966698647\n",
      "kl_divergence 0.005298307631164789\n",
      "reconstr_loss 0.01159356813877821\n",
      "kl_divergence 0.01176204439252615\n",
      "reconstr_loss 0.017719421535730362\n",
      "[epoch 27, i   20] loss: 0.006\n",
      "kl_divergence 0.008318847976624966\n",
      "reconstr_loss 0.0157471913844347\n",
      "kl_divergence 0.005794502329081297\n",
      "reconstr_loss 0.010826422832906246\n",
      "kl_divergence 0.005731106735765934\n",
      "reconstr_loss 0.012687492184340954\n",
      "kl_divergence 0.007201399188488722\n",
      "reconstr_loss 0.013344789855182171\n",
      "kl_divergence 0.010414917953312397\n",
      "reconstr_loss 0.016595616936683655\n",
      "kl_divergence 0.005971957463771105\n",
      "reconstr_loss 0.012139616534113884\n",
      "kl_divergence 0.0059782653115689754\n",
      "reconstr_loss 0.01081538200378418\n",
      "kl_divergence 0.007526319939643145\n",
      "reconstr_loss 0.011596315540373325\n",
      "kl_divergence 0.007501265965402126\n",
      "reconstr_loss 0.017178857699036598\n",
      "kl_divergence 0.008798479102551937\n",
      "reconstr_loss 0.0139379370957613\n",
      "kl_divergence 0.012865325435996056\n",
      "reconstr_loss 0.020648054778575897\n",
      "kl_divergence 0.0060575976967811584\n",
      "reconstr_loss 0.017117727547883987\n",
      "kl_divergence 0.003899540053680539\n",
      "reconstr_loss 0.014314997009932995\n",
      "kl_divergence 0.007859576493501663\n",
      "reconstr_loss 0.013853480108082294\n",
      "kl_divergence 0.006558452732861042\n",
      "reconstr_loss 0.019086306914687157\n",
      "kl_divergence 0.0054840692318975925\n",
      "reconstr_loss 0.013622078113257885\n",
      "kl_divergence 0.01036281231790781\n",
      "reconstr_loss 0.018550198525190353\n",
      "kl_divergence 0.005715311970561743\n",
      "reconstr_loss 0.01434275507926941\n",
      "kl_divergence 0.006408406421542168\n",
      "reconstr_loss 0.015300260856747627\n",
      "kl_divergence 0.004839366767555475\n",
      "reconstr_loss 0.010915218852460384\n",
      "kl_divergence 0.006211514584720135\n",
      "reconstr_loss 0.011461774818599224\n",
      "kl_divergence 0.008479088544845581\n",
      "reconstr_loss 0.01431176345795393\n",
      "kl_divergence 0.004509061574935913\n",
      "reconstr_loss 0.010046464391052723\n",
      "kl_divergence 0.006610301323235035\n",
      "reconstr_loss 0.01670501008629799\n",
      "kl_divergence 0.005889205262064934\n",
      "reconstr_loss 0.011362297460436821\n",
      "[epoch 28, i   20] loss: 0.005\n",
      "kl_divergence 0.00448872335255146\n",
      "reconstr_loss 0.011330991983413696\n",
      "kl_divergence 0.0060853804461658\n",
      "reconstr_loss 0.016314903274178505\n",
      "kl_divergence 0.006639308761805296\n",
      "reconstr_loss 0.014032943174242973\n",
      "kl_divergence 0.012644898146390915\n",
      "reconstr_loss 0.021165672689676285\n",
      "kl_divergence 0.006511520594358444\n",
      "reconstr_loss 0.017101431265473366\n",
      "kl_divergence 0.007586847059428692\n",
      "reconstr_loss 0.013760128989815712\n",
      "kl_divergence 0.004988859873265028\n",
      "reconstr_loss 0.013779657892882824\n",
      "kl_divergence 0.006595315411686897\n",
      "reconstr_loss 0.014821050688624382\n",
      "kl_divergence 0.00546089094132185\n",
      "reconstr_loss 0.015006961300969124\n",
      "kl_divergence 0.008550701662898064\n",
      "reconstr_loss 0.021693462505936623\n",
      "kl_divergence 0.0035476377233862877\n",
      "reconstr_loss 0.00968723464757204\n",
      "kl_divergence 0.004242016468197107\n",
      "reconstr_loss 0.011487631127238274\n",
      "kl_divergence 0.007035456132143736\n",
      "reconstr_loss 0.01632116734981537\n",
      "kl_divergence 0.005380894988775253\n",
      "reconstr_loss 0.0141901895403862\n",
      "kl_divergence 0.005429276265203953\n",
      "reconstr_loss 0.011981192044913769\n",
      "kl_divergence 0.004411526024341583\n",
      "reconstr_loss 0.012356676161289215\n",
      "kl_divergence 0.005232932977378368\n",
      "reconstr_loss 0.01408747024834156\n",
      "kl_divergence 0.013406096026301384\n",
      "reconstr_loss 0.020488549023866653\n",
      "kl_divergence 0.008182697929441929\n",
      "reconstr_loss 0.01493154652416706\n",
      "kl_divergence 0.007738575339317322\n",
      "reconstr_loss 0.014455961063504219\n",
      "kl_divergence 0.003980728331953287\n",
      "reconstr_loss 0.013327697291970253\n",
      "kl_divergence 0.005312422290444374\n",
      "reconstr_loss 0.014964130707085133\n",
      "kl_divergence 0.0073869964107871056\n",
      "reconstr_loss 0.01880667731165886\n",
      "kl_divergence 0.005429376848042011\n",
      "reconstr_loss 0.01844688132405281\n",
      "kl_divergence 0.004975913092494011\n",
      "reconstr_loss 0.01079932227730751\n",
      "[epoch 29, i   20] loss: 0.005\n",
      "kl_divergence 0.004616130143404007\n",
      "reconstr_loss 0.012169891968369484\n",
      "kl_divergence 0.00380230275914073\n",
      "reconstr_loss 0.013901147060096264\n",
      "kl_divergence 0.005144574213773012\n",
      "reconstr_loss 0.01262260228395462\n",
      "kl_divergence 0.0063859703950583935\n",
      "reconstr_loss 0.009882663376629353\n",
      "kl_divergence 0.006277056410908699\n",
      "reconstr_loss 0.01459532417356968\n",
      "kl_divergence 0.012321976013481617\n",
      "reconstr_loss 0.019220130518078804\n",
      "kl_divergence 0.004995127208530903\n",
      "reconstr_loss 0.01099284365773201\n",
      "kl_divergence 0.006217320915311575\n",
      "reconstr_loss 0.013984033837914467\n",
      "kl_divergence 0.004971952177584171\n",
      "reconstr_loss 0.01087193377315998\n",
      "kl_divergence 0.0047504180110991\n",
      "reconstr_loss 0.01167981792241335\n",
      "kl_divergence 0.006750816944986582\n",
      "reconstr_loss 0.010671677067875862\n",
      "kl_divergence 0.007205095607787371\n",
      "reconstr_loss 0.017207037657499313\n",
      "kl_divergence 0.007662116549909115\n",
      "reconstr_loss 0.01756000891327858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_divergence 0.004709359258413315\n",
      "reconstr_loss 0.011916193179786205\n",
      "kl_divergence 0.009375937283039093\n",
      "reconstr_loss 0.022347446531057358\n",
      "kl_divergence 0.004123569466173649\n",
      "reconstr_loss 0.0059328265488147736\n",
      "kl_divergence 0.00726691959425807\n",
      "reconstr_loss 0.013051694259047508\n",
      "kl_divergence 0.004863264504820108\n",
      "reconstr_loss 0.013097040355205536\n",
      "kl_divergence 0.004605230409651995\n",
      "reconstr_loss 0.01278116088360548\n",
      "kl_divergence 0.007218760903924704\n",
      "reconstr_loss 0.018640007823705673\n",
      "kl_divergence 0.0050712330266833305\n",
      "reconstr_loss 0.011542409658432007\n",
      "kl_divergence 0.004749146290123463\n",
      "reconstr_loss 0.014000256545841694\n",
      "kl_divergence 0.004864383023232222\n",
      "reconstr_loss 0.01658102311193943\n",
      "kl_divergence 0.004562929272651672\n",
      "reconstr_loss 0.013624632731080055\n",
      "kl_divergence 0.004004289861768484\n",
      "reconstr_loss 0.009741364046931267\n",
      "[epoch 30, i   20] loss: 0.005\n",
      "kl_divergence 0.006469270214438438\n",
      "reconstr_loss 0.01546388678252697\n",
      "kl_divergence 0.005157169885933399\n",
      "reconstr_loss 0.015365670435130596\n",
      "kl_divergence 0.0061529637314379215\n",
      "reconstr_loss 0.010737620294094086\n",
      "kl_divergence 0.006790344603359699\n",
      "reconstr_loss 0.011758965440094471\n",
      "kl_divergence 0.004113485570997\n",
      "reconstr_loss 0.012400595471262932\n",
      "kl_divergence 0.007046259939670563\n",
      "reconstr_loss 0.016029778867959976\n",
      "kl_divergence 0.004392718896269798\n",
      "reconstr_loss 0.01156413834542036\n",
      "kl_divergence 0.0044363271445035934\n",
      "reconstr_loss 0.01265427004545927\n",
      "kl_divergence 0.01661917194724083\n",
      "reconstr_loss 0.024218380451202393\n",
      "kl_divergence 0.006463152356445789\n",
      "reconstr_loss 0.01607273891568184\n",
      "kl_divergence 0.008336558006703854\n",
      "reconstr_loss 0.014624481089413166\n",
      "kl_divergence 0.0070995609275996685\n",
      "reconstr_loss 0.010611978359520435\n",
      "kl_divergence 0.004390668123960495\n",
      "reconstr_loss 0.014113659970462322\n",
      "kl_divergence 0.006847101263701916\n",
      "reconstr_loss 0.010404814034700394\n",
      "kl_divergence 0.009790251031517982\n",
      "reconstr_loss 0.021172266453504562\n",
      "kl_divergence 0.005422137677669525\n",
      "reconstr_loss 0.009721801616251469\n",
      "kl_divergence 0.005069781094789505\n",
      "reconstr_loss 0.014055753126740456\n",
      "kl_divergence 0.004893154837191105\n",
      "reconstr_loss 0.011754552833735943\n",
      "kl_divergence 0.004995487630367279\n",
      "reconstr_loss 0.011051657609641552\n",
      "kl_divergence 0.005969807505607605\n",
      "reconstr_loss 0.011460299603641033\n",
      "kl_divergence 0.008838645182549953\n",
      "reconstr_loss 0.014327215030789375\n",
      "kl_divergence 0.007369340397417545\n",
      "reconstr_loss 0.01332416944205761\n",
      "kl_divergence 0.0060255457647144794\n",
      "reconstr_loss 0.011735419742763042\n",
      "kl_divergence 0.01179639995098114\n",
      "reconstr_loss 0.01593940518796444\n",
      "kl_divergence 0.006299132015556097\n",
      "reconstr_loss 0.013260980136692524\n",
      "[epoch 31, i   20] loss: 0.005\n",
      "kl_divergence 0.004898085258901119\n",
      "reconstr_loss 0.01417824812233448\n",
      "kl_divergence 0.0040484704077243805\n",
      "reconstr_loss 0.014817577786743641\n",
      "kl_divergence 0.003897167043760419\n",
      "reconstr_loss 0.010379359126091003\n",
      "kl_divergence 0.004049524199217558\n",
      "reconstr_loss 0.013065114617347717\n",
      "kl_divergence 0.003558477619662881\n",
      "reconstr_loss 0.01202435977756977\n",
      "kl_divergence 0.005215168930590153\n",
      "reconstr_loss 0.011295055039227009\n",
      "kl_divergence 0.004249044694006443\n",
      "reconstr_loss 0.013564817607402802\n",
      "kl_divergence 0.005680995061993599\n",
      "reconstr_loss 0.010289197787642479\n",
      "kl_divergence 0.006163286976516247\n",
      "reconstr_loss 0.013393295928835869\n",
      "kl_divergence 0.0051004644483327866\n",
      "reconstr_loss 0.01533212698996067\n",
      "kl_divergence 0.004061208572238684\n",
      "reconstr_loss 0.014057020656764507\n",
      "kl_divergence 0.004553812090307474\n",
      "reconstr_loss 0.01042664609849453\n",
      "kl_divergence 0.01143375039100647\n",
      "reconstr_loss 0.022079186514019966\n",
      "kl_divergence 0.003877321258187294\n",
      "reconstr_loss 0.01385149359703064\n",
      "kl_divergence 0.005476038902997971\n",
      "reconstr_loss 0.013000558130443096\n",
      "kl_divergence 0.00710651371628046\n",
      "reconstr_loss 0.012311218306422234\n",
      "kl_divergence 0.0045993151143193245\n",
      "reconstr_loss 0.010388962924480438\n",
      "kl_divergence 0.003954234067350626\n",
      "reconstr_loss 0.011591914109885693\n",
      "kl_divergence 0.006046453025192022\n",
      "reconstr_loss 0.013117635622620583\n",
      "kl_divergence 0.004613359458744526\n",
      "reconstr_loss 0.01596922054886818\n",
      "kl_divergence 0.003667569486424327\n",
      "reconstr_loss 0.01170787401497364\n",
      "kl_divergence 0.00858128722757101\n",
      "reconstr_loss 0.018441390246152878\n",
      "kl_divergence 0.0041610561311244965\n",
      "reconstr_loss 0.012657958082854748\n",
      "kl_divergence 0.006801189389079809\n",
      "reconstr_loss 0.015243949368596077\n",
      "kl_divergence 0.004286270122975111\n",
      "reconstr_loss 0.01265176571905613\n",
      "[epoch 32, i   20] loss: 0.005\n",
      "kl_divergence 0.00640006922185421\n",
      "reconstr_loss 0.014852563850581646\n",
      "kl_divergence 0.004971188027411699\n",
      "reconstr_loss 0.014772109687328339\n",
      "kl_divergence 0.004930045921355486\n",
      "reconstr_loss 0.009265576489269733\n",
      "kl_divergence 0.005770575255155563\n",
      "reconstr_loss 0.011646383441984653\n",
      "kl_divergence 0.0064573949202895164\n",
      "reconstr_loss 0.018311047926545143\n",
      "kl_divergence 0.004609247203916311\n",
      "reconstr_loss 0.012478603050112724\n",
      "kl_divergence 0.0036131672095507383\n",
      "reconstr_loss 0.012276547029614449\n",
      "kl_divergence 0.006497133523225784\n",
      "reconstr_loss 0.01628684625029564\n",
      "kl_divergence 0.007157035171985626\n",
      "reconstr_loss 0.018644116818904877\n",
      "kl_divergence 0.004967928398400545\n",
      "reconstr_loss 0.011273644864559174\n",
      "kl_divergence 0.00466126250103116\n",
      "reconstr_loss 0.009344365447759628\n",
      "kl_divergence 0.0044038910418748856\n",
      "reconstr_loss 0.011716929264366627\n",
      "kl_divergence 0.0036979394499212503\n",
      "reconstr_loss 0.009115256369113922\n",
      "kl_divergence 0.004133020993322134\n",
      "reconstr_loss 0.014025340788066387\n",
      "kl_divergence 0.0049704513512551785\n",
      "reconstr_loss 0.013760698959231377\n",
      "kl_divergence 0.0035704465117305517\n",
      "reconstr_loss 0.011395822279155254\n",
      "kl_divergence 0.003667480079457164\n",
      "reconstr_loss 0.010558798909187317\n",
      "kl_divergence 0.005284057464450598\n",
      "reconstr_loss 0.010838298127055168\n",
      "kl_divergence 0.004915496334433556\n",
      "reconstr_loss 0.009963864460587502\n",
      "kl_divergence 0.0036690079141408205\n",
      "reconstr_loss 0.010481001809239388\n",
      "kl_divergence 0.005981797818094492\n",
      "reconstr_loss 0.019693685695528984\n",
      "kl_divergence 0.006647133734077215\n",
      "reconstr_loss 0.016846392303705215\n",
      "kl_divergence 0.005569804459810257\n",
      "reconstr_loss 0.011364980600774288\n",
      "kl_divergence 0.0069764144718647\n",
      "reconstr_loss 0.013570783659815788\n",
      "kl_divergence 0.0054961442947387695\n",
      "reconstr_loss 0.01625526323914528\n",
      "[epoch 33, i   20] loss: 0.005\n",
      "kl_divergence 0.005924544762820005\n",
      "reconstr_loss 0.014696848578751087\n",
      "kl_divergence 0.004835699684917927\n",
      "reconstr_loss 0.015148485079407692\n",
      "kl_divergence 0.006339454557746649\n",
      "reconstr_loss 0.01291636098176241\n",
      "kl_divergence 0.005064937751740217\n",
      "reconstr_loss 0.017660683020949364\n",
      "kl_divergence 0.004737926181405783\n",
      "reconstr_loss 0.012829323299229145\n",
      "kl_divergence 0.0034392487723380327\n",
      "reconstr_loss 0.008382492698729038\n",
      "kl_divergence 0.007891709916293621\n",
      "reconstr_loss 0.016625788062810898\n",
      "kl_divergence 0.003521642414852977\n",
      "reconstr_loss 0.015227062627673149\n",
      "kl_divergence 0.004405353218317032\n",
      "reconstr_loss 0.010697311721742153\n",
      "kl_divergence 0.009344653226435184\n",
      "reconstr_loss 0.019876204431056976\n",
      "kl_divergence 0.0038449435960501432\n",
      "reconstr_loss 0.010540775023400784\n",
      "kl_divergence 0.004782272968441248\n",
      "reconstr_loss 0.014421464875340462\n",
      "kl_divergence 0.004762310069054365\n",
      "reconstr_loss 0.011636155657470226\n",
      "kl_divergence 0.003212220035493374\n",
      "reconstr_loss 0.00933645199984312\n",
      "kl_divergence 0.0066505977883934975\n",
      "reconstr_loss 0.01753588393330574\n",
      "kl_divergence 0.004987281281501055\n",
      "reconstr_loss 0.00983520783483982\n",
      "kl_divergence 0.004929450806230307\n",
      "reconstr_loss 0.012580035254359245\n",
      "kl_divergence 0.007631405722349882\n",
      "reconstr_loss 0.018533404916524887\n",
      "kl_divergence 0.005982114002108574\n",
      "reconstr_loss 0.011033298447728157\n",
      "kl_divergence 0.004638229496777058\n",
      "reconstr_loss 0.01632707193493843\n",
      "kl_divergence 0.008710342459380627\n",
      "reconstr_loss 0.01772056147456169\n",
      "kl_divergence 0.005769414361566305\n",
      "reconstr_loss 0.017020361497998238\n",
      "kl_divergence 0.003922330681234598\n",
      "reconstr_loss 0.00914793275296688\n",
      "kl_divergence 0.005701314192265272\n",
      "reconstr_loss 0.014739426784217358\n",
      "kl_divergence 0.004034120123833418\n",
      "reconstr_loss 0.013232025317847729\n",
      "[epoch 34, i   20] loss: 0.005\n",
      "kl_divergence 0.005166095215827227\n",
      "reconstr_loss 0.011627648957073689\n",
      "kl_divergence 0.005687932018190622\n",
      "reconstr_loss 0.014337943866848946\n",
      "kl_divergence 0.007187020033597946\n",
      "reconstr_loss 0.015260371379554272\n",
      "kl_divergence 0.006463868543505669\n",
      "reconstr_loss 0.022367749363183975\n",
      "kl_divergence 0.0044746994972229\n",
      "reconstr_loss 0.010608399286866188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_divergence 0.005019557196646929\n",
      "reconstr_loss 0.008535796776413918\n",
      "kl_divergence 0.004724381025880575\n",
      "reconstr_loss 0.011714679189026356\n",
      "kl_divergence 0.004782667849212885\n",
      "reconstr_loss 0.013177583925426006\n",
      "kl_divergence 0.004841483198106289\n",
      "reconstr_loss 0.007883338257670403\n",
      "kl_divergence 0.0042897178791463375\n",
      "reconstr_loss 0.013532495126128197\n",
      "kl_divergence 0.005140599329024553\n",
      "reconstr_loss 0.017107928171753883\n",
      "kl_divergence 0.004439604934304953\n",
      "reconstr_loss 0.010128370486199856\n",
      "kl_divergence 0.004350291099399328\n",
      "reconstr_loss 0.012729229405522346\n",
      "kl_divergence 0.007259119302034378\n",
      "reconstr_loss 0.022364288568496704\n",
      "kl_divergence 0.00540502043440938\n",
      "reconstr_loss 0.01463076937943697\n",
      "kl_divergence 0.003724861890077591\n",
      "reconstr_loss 0.014364423230290413\n",
      "kl_divergence 0.003907463047653437\n",
      "reconstr_loss 0.00941853504627943\n",
      "kl_divergence 0.006329433061182499\n",
      "reconstr_loss 0.015107372775673866\n",
      "kl_divergence 0.004361479543149471\n",
      "reconstr_loss 0.010409492999315262\n",
      "kl_divergence 0.0034535559825599194\n",
      "reconstr_loss 0.015575818717479706\n",
      "kl_divergence 0.00459520285949111\n",
      "reconstr_loss 0.014081845059990883\n",
      "kl_divergence 0.0069955457001924515\n",
      "reconstr_loss 0.008180327713489532\n",
      "kl_divergence 0.00556982634589076\n",
      "reconstr_loss 0.01474085170775652\n",
      "kl_divergence 0.004232561215758324\n",
      "reconstr_loss 0.011897606775164604\n",
      "kl_divergence 0.007798937615007162\n",
      "reconstr_loss 0.01495913416147232\n",
      "[epoch 35, i   20] loss: 0.005\n",
      "kl_divergence 0.0038276631385087967\n",
      "reconstr_loss 0.00854278914630413\n",
      "kl_divergence 0.0038464334793388844\n",
      "reconstr_loss 0.015372524969279766\n",
      "kl_divergence 0.00469591747969389\n",
      "reconstr_loss 0.009976294822990894\n",
      "kl_divergence 0.003650638973340392\n",
      "reconstr_loss 0.014508885331451893\n",
      "kl_divergence 0.005249631125479937\n",
      "reconstr_loss 0.013766231946647167\n",
      "kl_divergence 0.007696699816733599\n",
      "reconstr_loss 0.020435987040400505\n",
      "kl_divergence 0.006340519525110722\n",
      "reconstr_loss 0.016043927520513535\n",
      "kl_divergence 0.003702681977301836\n",
      "reconstr_loss 0.01211505476385355\n",
      "kl_divergence 0.005276810377836227\n",
      "reconstr_loss 0.013519425876438618\n",
      "kl_divergence 0.006763317622244358\n",
      "reconstr_loss 0.022762037813663483\n",
      "kl_divergence 0.003977474290877581\n",
      "reconstr_loss 0.00947375874966383\n",
      "kl_divergence 0.0052384864538908005\n",
      "reconstr_loss 0.010789962485432625\n",
      "kl_divergence 0.008698420599102974\n",
      "reconstr_loss 0.02133886143565178\n",
      "kl_divergence 0.006674113683402538\n",
      "reconstr_loss 0.016078094020485878\n",
      "kl_divergence 0.003598954062908888\n",
      "reconstr_loss 0.01080093253403902\n",
      "kl_divergence 0.004249203018844128\n",
      "reconstr_loss 0.007066967897117138\n",
      "kl_divergence 0.004616020247340202\n",
      "reconstr_loss 0.016968779265880585\n",
      "kl_divergence 0.00469124224036932\n",
      "reconstr_loss 0.010307671502232552\n",
      "kl_divergence 0.0038252274971455336\n",
      "reconstr_loss 0.009653760120272636\n",
      "kl_divergence 0.0069754417054355145\n",
      "reconstr_loss 0.013441753573715687\n",
      "kl_divergence 0.0120741231366992\n",
      "reconstr_loss 0.023783180862665176\n",
      "kl_divergence 0.005525132175534964\n",
      "reconstr_loss 0.01485029049217701\n",
      "kl_divergence 0.004513058345764875\n",
      "reconstr_loss 0.009992632083594799\n",
      "kl_divergence 0.004957015626132488\n",
      "reconstr_loss 0.013508101925253868\n",
      "kl_divergence 0.005514700431376696\n",
      "reconstr_loss 0.011938802897930145\n",
      "[epoch 36, i   20] loss: 0.005\n",
      "kl_divergence 0.003656663466244936\n",
      "reconstr_loss 0.012970482930541039\n",
      "kl_divergence 0.004034693352878094\n",
      "reconstr_loss 0.009524969384074211\n",
      "kl_divergence 0.006284907925873995\n",
      "reconstr_loss 0.012794630602002144\n",
      "kl_divergence 0.007534685544669628\n",
      "reconstr_loss 0.018711913377046585\n",
      "kl_divergence 0.0036455532535910606\n",
      "reconstr_loss 0.015644604340195656\n",
      "kl_divergence 0.00382901169359684\n",
      "reconstr_loss 0.010232376866042614\n",
      "kl_divergence 0.006507576908916235\n",
      "reconstr_loss 0.014001932926476002\n",
      "kl_divergence 0.003824429353699088\n",
      "reconstr_loss 0.011408736929297447\n",
      "kl_divergence 0.0039040050469338894\n",
      "reconstr_loss 0.012004448100924492\n",
      "kl_divergence 0.0038177892565727234\n",
      "reconstr_loss 0.010054376907646656\n",
      "kl_divergence 0.00436189491301775\n",
      "reconstr_loss 0.011922964826226234\n",
      "kl_divergence 0.0034201329108327627\n",
      "reconstr_loss 0.007056474685668945\n",
      "kl_divergence 0.004091889597475529\n",
      "reconstr_loss 0.013586334884166718\n",
      "kl_divergence 0.004888338968157768\n",
      "reconstr_loss 0.014066172763705254\n",
      "kl_divergence 0.004132474306970835\n",
      "reconstr_loss 0.01446094922721386\n",
      "kl_divergence 0.003582760924473405\n",
      "reconstr_loss 0.010989556089043617\n",
      "kl_divergence 0.0057477098889648914\n",
      "reconstr_loss 0.014880245551466942\n",
      "kl_divergence 0.0037371418438851833\n",
      "reconstr_loss 0.00981858465820551\n",
      "kl_divergence 0.004918786231428385\n",
      "reconstr_loss 0.013177543878555298\n",
      "kl_divergence 0.003641836578026414\n",
      "reconstr_loss 0.016131656244397163\n",
      "kl_divergence 0.003930848091840744\n",
      "reconstr_loss 0.01678740233182907\n",
      "kl_divergence 0.004507578443735838\n",
      "reconstr_loss 0.010915422812104225\n",
      "kl_divergence 0.004069692920893431\n",
      "reconstr_loss 0.012775344774127007\n",
      "kl_divergence 0.00415128655731678\n",
      "reconstr_loss 0.014004671946167946\n",
      "kl_divergence 0.006012782454490662\n",
      "reconstr_loss 0.018777575343847275\n",
      "[epoch 37, i   20] loss: 0.004\n",
      "kl_divergence 0.007302199024707079\n",
      "reconstr_loss 0.015254922211170197\n",
      "kl_divergence 0.004409552551805973\n",
      "reconstr_loss 0.008153392001986504\n",
      "kl_divergence 0.0036923354491591454\n",
      "reconstr_loss 0.00754885096102953\n",
      "kl_divergence 0.004277138505131006\n",
      "reconstr_loss 0.01268885750323534\n",
      "kl_divergence 0.004959392827004194\n",
      "reconstr_loss 0.011826743371784687\n",
      "kl_divergence 0.0032759536989033222\n",
      "reconstr_loss 0.006939278449863195\n",
      "kl_divergence 0.005706527270376682\n",
      "reconstr_loss 0.015208845026791096\n",
      "kl_divergence 0.005171831697225571\n",
      "reconstr_loss 0.008874586783349514\n",
      "kl_divergence 0.007075751665979624\n",
      "reconstr_loss 0.018394526094198227\n",
      "kl_divergence 0.0036077818367630243\n",
      "reconstr_loss 0.01056643296033144\n",
      "kl_divergence 0.0036846355069428682\n",
      "reconstr_loss 0.012744712643325329\n",
      "kl_divergence 0.003431937424466014\n",
      "reconstr_loss 0.011002065613865852\n",
      "kl_divergence 0.004223396070301533\n",
      "reconstr_loss 0.013336606323719025\n",
      "kl_divergence 0.003949402831494808\n",
      "reconstr_loss 0.0105973482131958\n",
      "kl_divergence 0.004376470576971769\n",
      "reconstr_loss 0.011353308334946632\n",
      "kl_divergence 0.0036827714648097754\n",
      "reconstr_loss 0.011551806703209877\n",
      "kl_divergence 0.0032115434296429157\n",
      "reconstr_loss 0.008218370378017426\n",
      "kl_divergence 0.005794265307486057\n",
      "reconstr_loss 0.020024266093969345\n",
      "kl_divergence 0.005296205636113882\n",
      "reconstr_loss 0.01301172561943531\n",
      "kl_divergence 0.004166736267507076\n",
      "reconstr_loss 0.01680203154683113\n",
      "kl_divergence 0.0040765986777842045\n",
      "reconstr_loss 0.01767756976187229\n",
      "kl_divergence 0.00332311587408185\n",
      "reconstr_loss 0.00987621583044529\n",
      "kl_divergence 0.0032381683122366667\n",
      "reconstr_loss 0.00997098721563816\n",
      "kl_divergence 0.007430713158100843\n",
      "reconstr_loss 0.021076032891869545\n",
      "kl_divergence 0.005349867977201939\n",
      "reconstr_loss 0.009565973654389381\n",
      "[epoch 38, i   20] loss: 0.004\n",
      "kl_divergence 0.0038036396726965904\n",
      "reconstr_loss 0.013836948201060295\n",
      "kl_divergence 0.005501011852174997\n",
      "reconstr_loss 0.018260646611452103\n",
      "kl_divergence 0.003346127225086093\n",
      "reconstr_loss 0.007822243496775627\n",
      "kl_divergence 0.005612050648778677\n",
      "reconstr_loss 0.015035795979201794\n",
      "kl_divergence 0.0033917464315891266\n",
      "reconstr_loss 0.008858947083353996\n",
      "kl_divergence 0.0039389175362885\n",
      "reconstr_loss 0.012449463829398155\n",
      "kl_divergence 0.007422313094139099\n",
      "reconstr_loss 0.011823336593806744\n",
      "kl_divergence 0.0049837082624435425\n",
      "reconstr_loss 0.011232947930693626\n",
      "kl_divergence 0.004385421052575111\n",
      "reconstr_loss 0.012207153253257275\n",
      "kl_divergence 0.004709174856543541\n",
      "reconstr_loss 0.012567740865051746\n",
      "kl_divergence 0.0038154968060553074\n",
      "reconstr_loss 0.009768295101821423\n",
      "kl_divergence 0.005477845203131437\n",
      "reconstr_loss 0.013251284137368202\n",
      "kl_divergence 0.004160257987678051\n",
      "reconstr_loss 0.013323049061000347\n",
      "kl_divergence 0.0030690606217831373\n",
      "reconstr_loss 0.010411746799945831\n",
      "kl_divergence 0.003882266581058502\n",
      "reconstr_loss 0.01356208510696888\n",
      "kl_divergence 0.00379673158749938\n",
      "reconstr_loss 0.01225285604596138\n",
      "kl_divergence 0.004108690191060305\n",
      "reconstr_loss 0.01745017245411873\n",
      "kl_divergence 0.0033341962844133377\n",
      "reconstr_loss 0.012126595713198185\n",
      "kl_divergence 0.00337433279491961\n",
      "reconstr_loss 0.010019361972808838\n",
      "kl_divergence 0.005419858731329441\n",
      "reconstr_loss 0.01956888474524021\n",
      "kl_divergence 0.0040975697338581085\n",
      "reconstr_loss 0.010746432468295097\n",
      "kl_divergence 0.005115048494189978\n",
      "reconstr_loss 0.014767694287002087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_divergence 0.005310662090778351\n",
      "reconstr_loss 0.01049998588860035\n",
      "kl_divergence 0.008909713476896286\n",
      "reconstr_loss 0.012312081642448902\n",
      "kl_divergence 0.005448923911899328\n",
      "reconstr_loss 0.013020825572311878\n",
      "[epoch 39, i   20] loss: 0.004\n",
      "kl_divergence 0.006919957231730223\n",
      "reconstr_loss 0.020578769966959953\n",
      "kl_divergence 0.0039239441975951195\n",
      "reconstr_loss 0.013451464474201202\n",
      "kl_divergence 0.004023000597953796\n",
      "reconstr_loss 0.013028393499553204\n",
      "kl_divergence 0.004066803026944399\n",
      "reconstr_loss 0.014136743731796741\n",
      "kl_divergence 0.004275397397577763\n",
      "reconstr_loss 0.010966608300805092\n",
      "kl_divergence 0.0062020160257816315\n",
      "reconstr_loss 0.012870525941252708\n",
      "kl_divergence 0.006796988658607006\n",
      "reconstr_loss 0.01673351787030697\n",
      "kl_divergence 0.003320304211229086\n",
      "reconstr_loss 0.007624207995831966\n",
      "kl_divergence 0.004063034895807505\n",
      "reconstr_loss 0.018791213631629944\n",
      "kl_divergence 0.004046463407576084\n",
      "reconstr_loss 0.015682877972722054\n",
      "kl_divergence 0.0035236759576946497\n",
      "reconstr_loss 0.012866532430052757\n",
      "kl_divergence 0.0076497821137309074\n",
      "reconstr_loss 0.011845612898468971\n",
      "kl_divergence 0.0034166339319199324\n",
      "reconstr_loss 0.010420041158795357\n",
      "kl_divergence 0.0035773238632827997\n",
      "reconstr_loss 0.01369454525411129\n",
      "kl_divergence 0.003534207819029689\n",
      "reconstr_loss 0.009922956116497517\n",
      "kl_divergence 0.003375023603439331\n",
      "reconstr_loss 0.01450657844543457\n",
      "kl_divergence 0.003011528169736266\n",
      "reconstr_loss 0.01077982410788536\n",
      "kl_divergence 0.0033067313488572836\n",
      "reconstr_loss 0.011841671541333199\n",
      "kl_divergence 0.004194843117147684\n",
      "reconstr_loss 0.010518722236156464\n",
      "kl_divergence 0.0027443848084658384\n",
      "reconstr_loss 0.011105703189969063\n",
      "kl_divergence 0.0033905059099197388\n",
      "reconstr_loss 0.012609623372554779\n",
      "kl_divergence 0.00443482818081975\n",
      "reconstr_loss 0.012227481231093407\n",
      "kl_divergence 0.0030190811958163977\n",
      "reconstr_loss 0.007930994965136051\n",
      "kl_divergence 0.0032779125031083822\n",
      "reconstr_loss 0.009258383885025978\n",
      "kl_divergence 0.0039566210471093655\n",
      "reconstr_loss 0.013505019247531891\n",
      "[epoch 40, i   20] loss: 0.004\n",
      "kl_divergence 0.002907767426222563\n",
      "reconstr_loss 0.012770394794642925\n",
      "kl_divergence 0.003526411484926939\n",
      "reconstr_loss 0.010838829912245274\n",
      "kl_divergence 0.0033657285384833813\n",
      "reconstr_loss 0.010203086771070957\n",
      "kl_divergence 0.002944702748209238\n",
      "reconstr_loss 0.009538753889501095\n",
      "kl_divergence 0.0033549764193594456\n",
      "reconstr_loss 0.011500186286866665\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "running_loss = 0\n",
    "number_of_epochs = 40\n",
    "display_frq = 20\n",
    "\n",
    "number_of_layers = 5\n",
    "latent_size = 10\n",
    "\n",
    "vae = MyVAE(latent_size, number_of_layers)\n",
    "vae.train()\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = batch, shuffle = True, num_workers = 4)\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    for i, d in enumerate(train_loader, 0):\n",
    "        if len(d) < batch:\n",
    "            break\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        reconstr_x, x, mu, sigma = vae(d)\n",
    "\n",
    "        #print('reconstr_x: ', reconstr_x.shape)\n",
    "        #print('x:        : ', x.shape)\n",
    "\n",
    "        loss = vae.loss(reconstr_x.unsqueeze(-1), x.unsqueeze(-1), mu, sigma)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # append the end of epoch numbers for plotting\n",
    "        if i % len(train_loader) == 0:\n",
    "            losses.append([len(losses), running_loss])\n",
    "        # display te loss every 'display_frq' inputs\n",
    "        if i % display_frq == display_frq - 1:\n",
    "            print('[epoch %d, i %4d] loss: %.3f' % (epoch + 1, i+ 1, running_loss / 100))\n",
    "            running_loss = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.46547389]\n",
      " [3.34470052]\n",
      " [1.82217744]\n",
      " [1.06288402]\n",
      " [0.7717026 ]\n",
      " [0.49499393]\n",
      " [0.40182721]\n",
      " [0.35562488]\n",
      " [0.33934609]\n",
      " [0.25363054]\n",
      " [0.22803631]\n",
      " [0.2550359 ]\n",
      " [0.2444386 ]\n",
      " [0.20152735]\n",
      " [0.19746117]\n",
      " [0.19989804]\n",
      " [0.19930428]\n",
      " [0.15862055]\n",
      " [0.15633502]\n",
      " [0.15843542]\n",
      " [0.13648887]\n",
      " [0.14481891]\n",
      " [0.15421508]\n",
      " [0.13264657]\n",
      " [0.13224984]\n",
      " [0.14553559]\n",
      " [0.12803842]\n",
      " [0.12765864]\n",
      " [0.15053673]\n",
      " [0.12930985]\n",
      " [0.11112904]\n",
      " [0.11506048]\n",
      " [0.10466058]\n",
      " [0.1127058 ]\n",
      " [0.11229961]\n",
      " [0.11436282]\n",
      " [0.11590143]\n",
      " [0.09307668]\n",
      " [0.09485371]\n",
      " [0.10737514]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcd990799a0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.array(losses)[:,1:])\n",
    "plt.plot(np.array(losses)[:,1:])\n",
    "#vae.save('20d_10l_vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vae.load_state_dict(torch.load('1d_vae'))\n",
    "#vae.load_state_dict(torch.load('10d_20l_vae'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test, batch_size = batch, shuffle = True, num_workers = 4)\n",
    "data_iter = iter(test_loader)\n",
    "data = next(data_iter)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstr_x, x, mu, sigma = vae(data)\n",
    "    \n",
    "i = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-9a1789193729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'original data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'reconstructed data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original signal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reconstructed signal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2486\u001b[0m         data=None, **kwargs):\n\u001b[0;32m-> 2487\u001b[0;31m     return gca().bar(\n\u001b[0m\u001b[1;32m   2488\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2489\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[1;32m   2424\u001b[0m             \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2426\u001b[0;31m             \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_dx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2427\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0myerr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m                 \u001b[0myerr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_dx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36m_convert_dx\u001b[0;34m(dx, x0, xconv, convert)\u001b[0m\n\u001b[1;32m   2253\u001b[0m                 \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m                 \u001b[0mdelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m             \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mddx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mddx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2256\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdelist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m                 \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2253\u001b[0m                 \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m                 \u001b[0mdelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m             \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mddx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mddx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2256\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdelist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m                 \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i += 1\n",
    "\n",
    "ox = x[i].squeeze(-1)\n",
    "rx = reconstr_x[i]\n",
    "plt.figure(figsize=(16,8))\n",
    "# Plot the data\n",
    "xlin = np.arange(data_len)\n",
    "\n",
    "#plt.plot(xlin,rx,'.-r', label='reconstructed data')\n",
    "#plt.plot(xlin,ox,'.-', label='original data')\n",
    "\n",
    "plt.bar(xlin,ox, width=0.95, label='original data', alpha=0.25)\n",
    "plt.bar(xlin,rx, width=0.95, label='reconstructed data', alpha=0.5)\n",
    "\n",
    "plt.legend(['original signal', 'reconstructed signal']);\n",
    "plt.axvline(x=float(n_operators) - 0.5)\n",
    "plt.axvline(x=float(n_tables) - 0.5)\n",
    "\n",
    "print (ox.shape)\n",
    "print (rx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    INDEX = 0\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size = batch, shuffle = True, num_workers = 4)\n",
    "    data_iter = iter(test_loader)\n",
    "    # get one data pt\n",
    "    data = next(data_iter)\n",
    "\n",
    "    # get the lerp vals\n",
    "    lerp_offset = 1\n",
    "    lerp_range = 0.0\n",
    "    interp = [b * (1.0 / batch) * lerp_range + lerp_offset for b in range(batch)]\n",
    "\n",
    "    # get our latent space representations from the encoder\n",
    "    with torch.no_grad():\n",
    "        # encode\n",
    "        print('data shape:',data.shape)\n",
    "        mu, sigma = vae.encode(data)\n",
    "        # decode; lock a variable, permute, decode\n",
    "        z = vae.reparameterize(mu, sigma)\n",
    "\n",
    "    print(z.shape)\n",
    "    # get the first result; permute across the latent space\n",
    "    for i in range(batch):\n",
    "        z[i][INDEX] = interp[i]\n",
    "\n",
    "\n",
    "    # get ours reconstructions from the permuted latent spaces\n",
    "    with torch.no_grad():\n",
    "        results = vae.decode(z)\n",
    "\n",
    "    x = np.linspace(0,data_len,data_len)\n",
    "    y = np.linspace(0,batch,batch)\n",
    "\n",
    "    xc, yc = np.meshgrid(x, y)\n",
    "    res_surf = np.array(results)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    #YlOrRd\n",
    "    surf = ax.plot_surface(xc, yc, res_surf, cmap=\"coolwarm\",\n",
    "                           linewidth=0, antialiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    fig=plt.figure()\n",
    "    plt.ylim(top=15)\n",
    "\n",
    "    n=256 #Number of frames\n",
    "    x=range(56)\n",
    "    bars = plt.bar(x,results[0])\n",
    "\n",
    "    def animate(i):\n",
    "        for j, b in enumerate(bars):\n",
    "            b.set_height(results[i][j])\n",
    "\n",
    "    anim=animation.FuncAnimation(fig,animate,repeat=False,blit=False,frames=n,\n",
    "                                 interval=100)\n",
    "\n",
    "    anim.save('latent_perturbation.mp4',writer=animation.FFMpegWriter(fps=30))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "pt = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-embedding\n",
      "=============\n",
      "(close) similarity: 0.7550\n",
      "(far)   similarity: 0.2956\n",
      "\n",
      "post-embedding\n",
      "=============\n",
      "(close) similarity: 0.9688\n",
      "(far)   similarity: 0.3495\n"
     ]
    }
   ],
   "source": [
    "pt += 1\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    most_similar = 0\n",
    "    least_similar = None\n",
    "\n",
    "    # select a data point\n",
    "    d1 = data[pt]\n",
    "    d2 = None\n",
    "    d3 = None\n",
    "\n",
    "    vae.eval()\n",
    "\n",
    "    # get the closest and farthest vectors via cosine similarity\n",
    "    for i in range(1, len(data)):\n",
    "        similarity = cosine_sim(d1, data[i])\n",
    "        if similarity > most_similar and not np.isclose(1, similarity):\n",
    "            most_similar = similarity\n",
    "            d2 = data[i]\n",
    "        if least_similar is None or similarity < least_similar:\n",
    "            least_similar = similarity\n",
    "            d3 = data[i]\n",
    "\n",
    "    print(\"pre-embedding\")\n",
    "    print(\"=============\")\n",
    "    print(\"(close) similarity: %.4f\" % (cosine_sim(d1, d2)))\n",
    "    print(\"(far)   similarity: %.4f\" % (cosine_sim(d1, d3)))\n",
    "\n",
    "    # using the trained network, embed the points and\n",
    "    with torch.no_grad():\n",
    "        # get the embedded version of d1\n",
    "        d1 = torch.tensor(d1.reshape(1, data_len))\n",
    "        r1, _, _, _ = vae(d1)\n",
    "\n",
    "        d2 = torch.tensor(d2.reshape(1, data_len))\n",
    "        r2, _, _, _ = vae(d2)\n",
    "\n",
    "        d3 = torch.tensor(d3.reshape(1, data_len))\n",
    "        r3, _, _, _ = vae(d3)\n",
    "\n",
    "    d1r2 = cosine_sim(d1[0], r2[0])\n",
    "    d1r3 = cosine_sim(d1[0], r3[0])\n",
    "    print(\"\\npost-embedding\")\n",
    "    print(\"=============\")\n",
    "    print(\"(close) similarity: %.4f\" % d1r2)\n",
    "    print(\"(far)   similarity: %.4f\" % d1r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
